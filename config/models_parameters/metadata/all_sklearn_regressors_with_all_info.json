{
    "ARDRegression": {
        "regressor_info": "Bayesian ARD regression.\nFit the weights of a regression model, using an ARD prior. The weights of\nthe regression model are assumed to be in Gaussian distributions.\nAlso estimate the parameters lambda (precisions of the distributions of the\nweights) and alpha (precision of the distribution of the noise).\nThe estimation is done by an iterative procedures (Evidence Maximization)\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations. If None, it corresponds to max_iter=300.\n\nChanged in version 1.3.\n\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Stop the algorithm if w has converged.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "alpha_1",
                "description": "Hyper-parameter : shape parameter for the Gamma distribution prior\nover the alpha parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "alpha_2",
                "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the\nGamma distribution prior over the alpha parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "lambda_1",
                "description": "Hyper-parameter : shape parameter for the Gamma distribution prior\nover the lambda parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "lambda_2",
                "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the\nGamma distribution prior over the lambda parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "compute_score",
                "description": "If True, compute the objective function at each step of the model.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "threshold_lambda",
                "description": "Threshold for removing (pruning) weights with high precision from\nthe computation.\n",
                "value_default": "10000",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Verbose mode when fitting the model.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "n_iter",
                "description": "Maximum number of iterations.\n\nDeprecated since version 1.3: n_iter is deprecated in 1.3 and will be removed in 1.5. Use\nmax_iter instead.\n\n",
                "value_default": null,
                "data_type": "int"
            }
        ],
        "references": [
            "D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\ncompetition, ASHRAE Transactions, 1994.",
            "R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\nhttp://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\nTheir beta is our self.alpha_\nTheir alpha is our self.lambda_\nARD is a little different than the slide: only dimensions/features for\nwhich self.lambda_ < self.threshold_lambda are kept and the rest are\ndiscarded."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html"
    },
    "AdaBoostRegressor": {
        "regressor_info": "An AdaBoost regressor.\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\nThis class implements the algorithm known as AdaBoost.R2 [2].\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "estimator",
                "description": "The base estimator from which the boosted ensemble is built.\nIf None, then the base estimator is\nDecisionTreeRegressor initialized with\nmax_depth=3.\n\nNew in version 1.2: base_estimator was renamed to estimator.\n\n",
                "value_default": "None",
                "data_type": "object"
            },
            {
                "parameter": "n_estimators",
                "description": "The maximum number of estimators at which boosting is terminated.\nIn case of perfect fit, the learning procedure is stopped early.\nValues must be in the range [1, inf).\n",
                "value_default": "50",
                "data_type": "int"
            },
            {
                "parameter": "learning_rate",
                "description": "Weight applied to each regressor at each boosting iteration. A higher\nlearning rate increases the contribution of each regressor. There is\na trade-off between the learning_rate and n_estimators parameters.\nValues must be in the range (0.0, inf).\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "loss",
                "description": "The loss function to use when updating the weights after each\nboosting iteration.\n",
                "value_default": "linear",
                "data_type": "{linear, square, exponential}"
            },
            {
                "parameter": "random_state",
                "description": "Controls the random seed given at each estimator at each\nboosting iteration.\nThus, it is only used when estimator exposes a random_state.\nIn addition, it controls the bootstrap of the weights used to train the\nestimator at each boosting iteration.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            }
        ],
        "references": [
            "[1]\nY. Freund, R. Schapire, \u201cA Decision-Theoretic Generalization of\non-Line Learning and an Application to Boosting\u201d, 1995.",
            "[2]\n\nDrucker, \u201cImproving Regressors using Boosting Techniques\u201d, 1997."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html"
    },
    "BaggingRegressor": {
        "regressor_info": "A Bagging regressor.\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]. If samples are drawn with\nreplacement, then the method is known as Bagging [2]. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4].\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "estimator",
                "description": "The base estimator to fit on random subsets of the dataset.\nIf None, then the base estimator is a\nDecisionTreeRegressor.\n\nNew in version 1.2: base_estimator was renamed to estimator.\n\n",
                "value_default": "None",
                "data_type": "object"
            },
            {
                "parameter": "n_estimators",
                "description": "The number of base estimators in the ensemble.\n",
                "value_default": "10",
                "data_type": "int"
            },
            {
                "parameter": "max_samples",
                "description": "The number of samples to draw from X to train each base estimator (with\nreplacement by default, see bootstrap for more details).\n\nIf int, then draw max_samples samples.\nIf float, then draw max_samples * X.shape[0] samples.\n\n",
                "value_default": "1.0",
                "data_type": "int or float"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to draw from X to train each base estimator (\nwithout replacement by default, see bootstrap_features for more\ndetails).\n\nIf int, then draw max_features features.\nIf float, then draw max(1, int(max_features * n_features_in_)) features.\n\n",
                "value_default": "1.0",
                "data_type": "int or float"
            },
            {
                "parameter": "bootstrap",
                "description": "Whether samples are drawn with replacement. If False, sampling\nwithout replacement is performed.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "bootstrap_features",
                "description": "Whether features are drawn with replacement.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "oob_score",
                "description": "Whether to use out-of-bag samples to estimate\nthe generalization error. Only available if bootstrap=True.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit\na whole new ensemble. See the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of jobs to run in parallel for both fit and\npredict. None means 1 unless in a\njoblib.parallel_backend context. -1 means using all\nprocessors. See Glossary for more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Controls the random resampling of the original dataset\n(sample wise and feature wise).\nIf the base estimator accepts a random_state attribute, a different\nseed is generated for each instance in the ensemble.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "verbose",
                "description": "Controls the verbosity when fitting and predicting.\n",
                "value_default": "0",
                "data_type": "int"
            }
        ],
        "references": [
            "[1]\nL. Breiman, \u201cPasting small votes for classification in large\ndatabases and on-line\u201d, Machine Learning, 36(1), 85-103, 1999.",
            "[2]\nL. Breiman, \u201cBagging predictors\u201d, Machine Learning, 24(2), 123-140,\n1996.",
            "[3]\nT. Ho, \u201cThe random subspace method for constructing decision\nforests\u201d, Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n1998.",
            "[4]\nG. Louppe and P. Geurts, \u201cEnsembles on Random Patches\u201d, Machine\nLearning and Knowledge Discovery in Databases, 346-361, 2012."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html"
    },
    "BayesianRidge": {
        "regressor_info": "Bayesian ridge regression.\nFit a Bayesian ridge model. See the Notes section for details on this\nimplementation and the optimization of the regularization parameters\nlambda (precision of the weights) and alpha (precision of the noise).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations over the complete dataset before\nstopping independently of any early stopping criterion. If None, it\ncorresponds to max_iter=300.\n\nChanged in version 1.3.\n\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Stop the algorithm if w has converged.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "alpha_1",
                "description": "Hyper-parameter : shape parameter for the Gamma distribution prior\nover the alpha parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "alpha_2",
                "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the\nGamma distribution prior over the alpha parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "lambda_1",
                "description": "Hyper-parameter : shape parameter for the Gamma distribution prior\nover the lambda parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "lambda_2",
                "description": "Hyper-parameter : inverse scale parameter (rate parameter) for the\nGamma distribution prior over the lambda parameter.\n",
                "value_default": "1e-6",
                "data_type": "float"
            },
            {
                "parameter": "alpha_init",
                "description": "Initial value for alpha (precision of the noise).\nIf not set, alpha_init is 1/Var(y).\n\n\nNew in version 0.22.\n\n\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "lambda_init",
                "description": "Initial value for lambda (precision of the weights).\nIf not set, lambda_init is 1.\n\n\nNew in version 0.22.\n\n\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "compute_score",
                "description": "If True, compute the log marginal likelihood at each iteration of the\noptimization.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model.\nThe intercept is not treated as a probabilistic parameter\nand thus has no associated variance. If set\nto False, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Verbose mode when fitting the model.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "n_iter",
                "description": "Maximum number of iterations. Should be greater than or equal to 1.\n\nDeprecated since version 1.3: n_iter is deprecated in 1.3 and will be removed in 1.5. Use\nmax_iter instead.\n\n",
                "value_default": null,
                "data_type": "int"
            }
        ],
        "references": [
            "D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\nVol. 4, No. 3, 1992.",
            "M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\nJournal of Machine Learning Research, Vol. 1, 2001."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html"
    },
    "CCA": {
        "regressor_info": "Canonical Correlation Analysis, also known as \u201cMode B\u201d PLS.\nFor a comparison between other cross decomposition algorithms, see\nCompare cross decomposition methods.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_components",
                "description": "Number of components to keep. Should be in [1, min(n_samples,\nn_features, n_targets)].\n",
                "value_default": "2",
                "data_type": "int"
            },
            {
                "parameter": "scale",
                "description": "Whether to scale X and Y.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations of the power method.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of u_i - u_{i-1} is less\nthan tol, where u corresponds to the left singular vector.\n",
                "value_default": "1e-06",
                "data_type": "float"
            },
            {
                "parameter": "copy",
                "description": "Whether to copy X and Y in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays.\n",
                "value_default": "True",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html"
    },
    "DecisionTreeRegressor": {
        "regressor_info": "A decision tree regressor.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "criterion",
                "description": "The function to measure the quality of a split. Supported criteria\nare \u201csquared_error\u201d for the mean squared error, which is equal to\nvariance reduction as feature selection criterion and minimizes the L2\nloss using the mean of each terminal node, \u201cfriedman_mse\u201d, which uses\nmean squared error with Friedman\u2019s improvement score for potential\nsplits, \u201cabsolute_error\u201d for the mean absolute error, which minimizes\nthe L1 loss using the median of each terminal node, and \u201cpoisson\u201d which\nuses reduction in Poisson deviance to find splits.\n\nNew in version 0.18: Mean Absolute Error (MAE) criterion.\n\n\nNew in version 0.24: Poisson deviance criterion.\n\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, friedman_mse, absolute_error,             poisson}"
            },
            {
                "parameter": "splitter",
                "description": "The strategy used to choose the split at each node. Supported\nstrategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose\nthe best random split.\n",
                "value_default": "best",
                "data_type": "{best, random}"
            },
            {
                "parameter": "max_depth",
                "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_samples_split",
                "description": "The minimum number of samples required to split an internal node:\n\nIf int, then consider min_samples_split as the minimum number.\nIf float, then min_samples_split is a fraction and\nceil(min_samples_split * n_samples) are the minimum\nnumber of samples for each split.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "2",
                "data_type": "int or float"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast min_samples_leaf training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\nIf float, then min_samples_leaf is a fraction and\nceil(min_samples_leaf * n_samples) are the minimum\nnumber of samples for each node.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "1",
                "data_type": "int or float"
            },
            {
                "parameter": "min_weight_fraction_leaf",
                "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to consider when looking for the best split:\n\nIf int, then consider max_features features at each split.\nIf float, then max_features is a fraction and\nmax(1, int(max_features * n_features_in_)) features are considered at each\nsplit.\nIf \u201csqrt\u201d, then max_features=sqrt(n_features).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None, then max_features=n_features.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than max_features features.\n",
                "value_default": "None",
                "data_type": "int, float or {sqrt, log2}"
            },
            {
                "parameter": "random_state",
                "description": "Controls the randomness of the estimator. The features are always\nrandomly permuted at each split, even if splitter is set to\n\"best\". When max_features < n_features, the algorithm will\nselect max_features at random at each split before finding the best\nsplit among them. But the best found split may vary across different\nruns, even if max_features=n_features. That is the case, if the\nimprovement of the criterion is identical for several splits and one\nsplit has to be selected at random. To obtain a deterministic behaviour\nduring fitting, random_state has to be fixed to an integer.\nSee Glossary for details.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "Grow a tree with max_leaf_nodes in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_impurity_decrease",
                "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following:\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n                    - N_t_L / N_t * left_impurity)\n\n\nwhere N is the total number of samples, N_t is the number of\nsamples at the current node, N_t_L is the number of samples in the\nleft child, and N_t_R is the number of samples in the right child.\nN, N_t, N_t_R and N_t_L all refer to the weighted sum,\nif sample_weight is passed.\n\nNew in version 0.19.\n\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "ccp_alpha",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\nccp_alpha will be chosen. By default, no pruning is performed. See\nMinimal Cost-Complexity Pruning for details.\n\nNew in version 0.22.\n\n",
                "value_default": "0.0",
                "data_type": "non-negative float"
            }
        ],
        "references": [
            "[1]\nhttps://en.wikipedia.org/wiki/Decision_tree_learning",
            "[2]\nL. Breiman, J. Friedman, R. Olshen, and C. Stone, \u201cClassification\nand Regression Trees\u201d, Wadsworth, Belmont, CA, 1984.",
            "[3]\nT. Hastie, R. Tibshirani and J. Friedman. \u201cElements of Statistical\nLearning\u201d, Springer, 2009.",
            "[4]\nL. Breiman, and A. Cutler, \u201cRandom Forests\u201d,\nhttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
    },
    "DummyRegressor": {
        "regressor_info": "Regressor that makes predictions using simple rules.\nThis regressor is useful as a simple baseline to compare with other\n(real) regressors. Do not use it for real problems.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "strategy",
                "description": "Strategy to use to generate predictions.\n\n\u201cmean\u201d: always predicts the mean of the training set\n\u201cmedian\u201d: always predicts the median of the training set\n\u201cquantile\u201d: always predicts a specified quantile of the training set,\nprovided with the quantile parameter.\n\u201cconstant\u201d: always predicts a constant value that is provided by\nthe user.\n\n",
                "value_default": "mean",
                "data_type": "{mean, median, quantile, constant}"
            },
            {
                "parameter": "constant",
                "description": "The explicit constant as predicted by the \u201cconstant\u201d strategy. This\nparameter is useful only for the \u201cconstant\u201d strategy.\n",
                "value_default": "None",
                "data_type": "int or float or array-like of shape (n_outputs,)"
            },
            {
                "parameter": "quantile",
                "description": "The quantile to predict using the \u201cquantile\u201d strategy. A quantile of\n0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the\nmaximum.\n",
                "value_default": "None",
                "data_type": "float in [0.0, 1.0]"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html"
    },
    "ElasticNet": {
        "regressor_info": "Linear regression with combined L1 and L2 priors as regularizer.\nMinimizes the objective function:\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to:\nwhere:\nThe parameter l1_ratio corresponds to alpha in the glmnet R package while\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\nunless you supply your own sequence of alpha.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the penalty terms. Defaults to 1.0.\nSee the notes for the exact mathematical meaning of this\nparameter. alpha = 0 is equivalent to an ordinary least square,\nsolved by the LinearRegression object. For numerical\nreasons, using alpha = 0 with the Lasso object is not advised.\nGiven this, you should use the LinearRegression object.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "l1_ratio",
                "description": "The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. For\nl1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it\nis an L1 penalty.  For 0 < l1_ratio < 1, the penalty is a\ncombination of L1 and L2.\n",
                "value_default": "0.5",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether the intercept should be estimated or not. If False, the\ndata is assumed to be already centered.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. The Gram matrix can also be passed as argument.\nFor sparse input this option is always False to preserve sparsity.\n",
                "value_default": "False",
                "data_type": "bool or array-like of shape (n_features, n_features),                 default=False"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol, see Notes below.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "positive",
                "description": "When set to True, forces the coefficients to be positive.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
    },
    "ElasticNetCV": {
        "regressor_info": "Elastic Net model with iterative fitting along a regularization path.\nSee glossary entry for cross-validation estimator.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "l1_ratio",
                "description": "Float between 0 and 1 passed to ElasticNet (scaling between\nl1 and l2 penalties). For l1_ratio = 0\nthe penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\nFor 0 < l1_ratio < 1, the penalty is a combination of L1 and L2\nThis parameter can be a list, in which case the different\nvalues are tested by cross-validation and the one giving the best\nprediction score is used. Note that a good choice of list of\nvalues for l1_ratio is often to put more values close to 1\n(i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1, .5, .7,\n.9, .95, .99, 1].\n",
                "value_default": "0.5",
                "data_type": "float or list of float"
            },
            {
                "parameter": "eps",
                "description": "Length of the path. eps=1e-3 means that\nalpha_min / alpha_max = 1e-3.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "n_alphas",
                "description": "Number of alphas along the regularization path, used for each l1_ratio.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "alphas",
                "description": "List of alphas where to compute the models.\nIf None alphas are set automatically.\n",
                "value_default": "None",
                "data_type": "array-like"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram\nmatrix can also be passed as argument.\n",
                "value_default": "auto",
                "data_type": "auto, bool or array-like of shape             (n_features, n_features)"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\nint, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor int/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or iterable"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Amount of verbosity.\n",
                "value_default": "0",
                "data_type": "bool or int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "positive",
                "description": "When set to True, forces the coefficients to be positive.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html"
    },
    "ExtraTreeRegressor": {
        "regressor_info": "An extremely randomized tree regressor.\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the max_features randomly\nselected features and the best split among those is chosen. When\nmax_features is set 1, this amounts to building a totally random\ndecision tree.\nWarning: Extra-trees should only be used within ensemble methods.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "criterion",
                "description": "The function to measure the quality of a split. Supported criteria\nare \u201csquared_error\u201d for the mean squared error, which is equal to\nvariance reduction as feature selection criterion and minimizes the L2\nloss using the mean of each terminal node, \u201cfriedman_mse\u201d, which uses\nmean squared error with Friedman\u2019s improvement score for potential\nsplits, \u201cabsolute_error\u201d for the mean absolute error, which minimizes\nthe L1 loss using the median of each terminal node, and \u201cpoisson\u201d which\nuses reduction in Poisson deviance to find splits.\n\nNew in version 0.18: Mean Absolute Error (MAE) criterion.\n\n\nNew in version 0.24: Poisson deviance criterion.\n\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, friedman_mse, absolute_error, poisson},             default=squared_error"
            },
            {
                "parameter": "splitter",
                "description": "The strategy used to choose the split at each node. Supported\nstrategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose\nthe best random split.\n",
                "value_default": "random",
                "data_type": "{random, best}"
            },
            {
                "parameter": "max_depth",
                "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_samples_split",
                "description": "The minimum number of samples required to split an internal node:\n\nIf int, then consider min_samples_split as the minimum number.\nIf float, then min_samples_split is a fraction and\nceil(min_samples_split * n_samples) are the minimum\nnumber of samples for each split.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "2",
                "data_type": "int or float"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast min_samples_leaf training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\nIf float, then min_samples_leaf is a fraction and\nceil(min_samples_leaf * n_samples) are the minimum\nnumber of samples for each node.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "1",
                "data_type": "int or float"
            },
            {
                "parameter": "min_weight_fraction_leaf",
                "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to consider when looking for the best split:\n\nIf int, then consider max_features features at each split.\nIf float, then max_features is a fraction and\nmax(1, int(max_features * n_features_in_)) features are considered at each\nsplit.\nIf \u201csqrt\u201d, then max_features=sqrt(n_features).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None, then max_features=n_features.\n\n\nChanged in version 1.1: The default of max_features changed from \"auto\" to 1.0.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than max_features features.\n",
                "value_default": "1.0",
                "data_type": "int, float, {sqrt, log2} or None"
            },
            {
                "parameter": "random_state",
                "description": "Used to pick randomly the max_features used at each split.\nSee Glossary for details.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "min_impurity_decrease",
                "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following:\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n                    - N_t_L / N_t * left_impurity)\n\n\nwhere N is the total number of samples, N_t is the number of\nsamples at the current node, N_t_L is the number of samples in the\nleft child, and N_t_R is the number of samples in the right child.\nN, N_t, N_t_R and N_t_L all refer to the weighted sum,\nif sample_weight is passed.\n\nNew in version 0.19.\n\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "Grow a tree with max_leaf_nodes in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "ccp_alpha",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\nccp_alpha will be chosen. By default, no pruning is performed. See\nMinimal Cost-Complexity Pruning for details.\n\nNew in version 0.22.\n\n",
                "value_default": "0.0",
                "data_type": "non-negative float"
            }
        ],
        "references": [
            "[1]\nP. Geurts, D. Ernst., and L. Wehenkel, \u201cExtremely randomized trees\u201d,\nMachine Learning, 63(1), 3-42, 2006."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html"
    },
    "ExtraTreesRegressor": {
        "regressor_info": "An extra-trees regressor.\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and uses averaging to improve the predictive accuracy\nand control over-fitting.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_estimators",
                "description": "The number of trees in the forest.\n\nChanged in version 0.22: The default value of n_estimators changed from 10 to 100\nin 0.22.\n\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "criterion",
                "description": "The function to measure the quality of a split. Supported criteria\nare \u201csquared_error\u201d for the mean squared error, which is equal to\nvariance reduction as feature selection criterion and minimizes the L2\nloss using the mean of each terminal node, \u201cfriedman_mse\u201d, which uses\nmean squared error with Friedman\u2019s improvement score for potential\nsplits, \u201cabsolute_error\u201d for the mean absolute error, which minimizes\nthe L1 loss using the median of each terminal node, and \u201cpoisson\u201d which\nuses reduction in Poisson deviance to find splits.\nTraining using \u201cabsolute_error\u201d is significantly slower\nthan when using \u201csquared_error\u201d.\n\nNew in version 0.18: Mean Absolute Error (MAE) criterion.\n\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, absolute_error, friedman_mse, poisson},             default=squared_error"
            },
            {
                "parameter": "max_depth",
                "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_samples_split",
                "description": "The minimum number of samples required to split an internal node:\n\nIf int, then consider min_samples_split as the minimum number.\nIf float, then min_samples_split is a fraction and\nceil(min_samples_split * n_samples) are the minimum\nnumber of samples for each split.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "2",
                "data_type": "int or float"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast min_samples_leaf training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\nIf float, then min_samples_leaf is a fraction and\nceil(min_samples_leaf * n_samples) are the minimum\nnumber of samples for each node.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "1",
                "data_type": "int or float"
            },
            {
                "parameter": "min_weight_fraction_leaf",
                "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to consider when looking for the best split:\n\nIf int, then consider max_features features at each split.\nIf float, then max_features is a fraction and\nmax(1, int(max_features * n_features_in_)) features are considered at each\nsplit.\nIf \u201csqrt\u201d, then max_features=sqrt(n_features).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None or 1.0, then max_features=n_features.\n\n\nNote\nThe default of 1.0 is equivalent to bagged trees and more\nrandomness can be achieved by setting smaller values, e.g. 0.3.\n\n\nChanged in version 1.1: The default of max_features changed from \"auto\" to 1.0.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than max_features features.\n",
                "value_default": "1.0",
                "data_type": "{sqrt, log2, None}, int or float"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "Grow trees with max_leaf_nodes in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_impurity_decrease",
                "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following:\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n                    - N_t_L / N_t * left_impurity)\n\n\nwhere N is the total number of samples, N_t is the number of\nsamples at the current node, N_t_L is the number of samples in the\nleft child, and N_t_R is the number of samples in the right child.\nN, N_t, N_t_R and N_t_L all refer to the weighted sum,\nif sample_weight is passed.\n\nNew in version 0.19.\n\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "bootstrap",
                "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "oob_score",
                "description": "Whether to use out-of-bag samples to estimate the generalization score.\nBy default, r2_score is used.\nProvide a callable with signature metric(y_true, y_pred) to use a\ncustom metric. Only available if bootstrap=True.\n",
                "value_default": "False",
                "data_type": "bool or callable"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of jobs to run in parallel. fit, predict,\ndecision_path and apply are all parallelized over the\ntrees. None means 1 unless in a joblib.parallel_backend\ncontext. -1 means using all processors. See Glossary for more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Controls 3 sources of randomness:\n\nthe bootstrapping of the samples used when building trees\n(if bootstrap=True)\nthe sampling of the features to consider when looking for the best\nsplit at each node (if max_features < n_features)\nthe draw of the splits for each of the max_features\n\nSee Glossary for details.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "verbose",
                "description": "Controls the verbosity when fitting and predicting.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See Glossary and\nFitting additional weak-learners for details.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "ccp_alpha",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\nccp_alpha will be chosen. By default, no pruning is performed. See\nMinimal Cost-Complexity Pruning for details.\n\nNew in version 0.22.\n\n",
                "value_default": "0.0",
                "data_type": "non-negative float"
            },
            {
                "parameter": "max_samples",
                "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\nIf None (default), then draw X.shape[0] samples.\nIf int, then draw max_samples samples.\nIf float, then draw max_samples * X.shape[0] samples. Thus,\nmax_samples should be in the interval (0.0, 1.0].\n\n\nNew in version 0.22.\n\n",
                "value_default": "None",
                "data_type": "int or float"
            }
        ],
        "references": [
            "[1]\nP. Geurts, D. Ernst., and L. Wehenkel, \u201cExtremely randomized trees\u201d,\nMachine Learning, 63(1), 3-42, 2006."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html"
    },
    "GammaRegressor": {
        "regressor_info": "Generalized Linear Model with a Gamma distribution.\nThis regressor uses the \u2018log\u2019 link function.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L2 penalty term and determines the\nregularization strength. alpha = 0 is equivalent to unpenalized\nGLMs. In this case, the design matrix X must have full column rank\n(no collinearities).\nValues of alpha must be in the range [0.0, inf).\n",
                "value_default": "1",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Specifies if a constant (a.k.a. bias or intercept) should be\nadded to the linear predictor X @ coef_ + intercept_.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "solver",
                "description": "Algorithm to use in the optimization problem:\n\n\u2018lbfgs\u2019Calls scipy\u2019s L-BFGS-B optimizer.\n\n\u2018newton-cholesky\u2019Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\niterated reweighted least squares) with an inner Cholesky based solver.\nThis solver is a good choice for n_samples >> n_features, especially\nwith one-hot encoded categorical features with rare categories. Be aware\nthat the memory usage of this solver has a quadratic dependency on\nn_features because it explicitly computes the Hessian matrix.\n\nNew in version 1.2.\n\n\n\n",
                "value_default": "lbfgs",
                "data_type": "{lbfgs, newton-cholesky}"
            },
            {
                "parameter": "max_iter",
                "description": "The maximal number of iterations for the solver.\nValues must be in the range [1, inf).\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Stopping criterion. For the lbfgs solver,\nthe iteration will stop when max{|g_j|, j = 1, ..., d} <= tol\nwhere g_j is the j-th component of the gradient (derivative) of\nthe objective function.\nValues must be in the range (0.0, inf).\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "If set to True, reuse the solution of the previous call to fit\nas initialization for coef_ and intercept_.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "For the lbfgs solver set verbose to any positive number for verbosity.\nValues must be in the range [0, inf).\n",
                "value_default": "0",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.GammaRegressor.html"
    },
    "GaussianProcessRegressor": {
        "regressor_info": "Gaussian process regression (GPR).\nThe implementation is based on Algorithm 2.1 of [RW2006].\nIn addition to standard scikit-learn estimator API,\nGaussianProcessRegressor:\nTo learn the difference between a point-estimate approach vs. a more\nBayesian modelling approach, refer to the example entitled\nComparison of kernel ridge and Gaussian process regression.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "kernel",
                "description": "The kernel specifying the covariance function of the GP. If None is\npassed, the kernel ConstantKernel(1.0, constant_value_bounds=\"fixed\")\n* RBF(1.0, length_scale_bounds=\"fixed\") is used as default. Note that\nthe kernel hyperparameters are optimized during fitting unless the\nbounds are marked as \u201cfixed\u201d.\n",
                "value_default": "None",
                "data_type": "kernel instance"
            },
            {
                "parameter": "alpha",
                "description": "Value added to the diagonal of the kernel matrix during fitting.\nThis can prevent a potential numerical issue during fitting, by\nensuring that the calculated values form a positive definite matrix.\nIt can also be interpreted as the variance of additional Gaussian\nmeasurement noise on the training observations. Note that this is\ndifferent from using a WhiteKernel. If an array is passed, it must\nhave the same number of entries as the data used for fitting and is\nused as datapoint-dependent noise level. Allowing to specify the\nnoise level directly as a parameter is mainly for convenience and\nfor consistency with Ridge.\n",
                "value_default": "1e-10",
                "data_type": "float or ndarray of shape (n_samples,)"
            },
            {
                "parameter": "optimizer",
                "description": "Can either be one of the internally supported optimizers for optimizing\nthe kernel\u2019s parameters, specified by a string, or an externally\ndefined optimizer passed as a callable. If a callable is passed, it\nmust have the signature:\ndef optimizer(obj_func, initial_theta, bounds):\n    # * 'obj_func': the objective function to be minimized, which\n    #   takes the hyperparameters theta as a parameter and an\n    #   optional flag eval_gradient, which determines if the\n    #   gradient is returned additionally to the function value\n    # * 'initial_theta': the initial value for theta, which can be\n    #   used by local optimizers\n    # * 'bounds': the bounds on the values of theta\n    ....\n    # Returned are the best found hyperparameters theta and\n    # the corresponding value of the target function.\n    return theta_opt, func_min\n\n\nPer default, the L-BFGS-B algorithm from scipy.optimize.minimize\nis used. If None is passed, the kernel\u2019s parameters are kept fixed.\nAvailable internal optimizers are: {'fmin_l_bfgs_b'}.\n",
                "value_default": "fmin_l_bfgs_b",
                "data_type": "fmin_l_bfgs_b, callable or None"
            },
            {
                "parameter": "n_restarts_optimizer",
                "description": "The number of restarts of the optimizer for finding the kernel\u2019s\nparameters which maximize the log-marginal likelihood. The first run\nof the optimizer is performed from the kernel\u2019s initial parameters,\nthe remaining ones (if any) from thetas sampled log-uniform randomly\nfrom the space of allowed theta-values. If greater than 0, all bounds\nmust be finite. Note that n_restarts_optimizer == 0 implies that one\nrun is performed.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "normalize_y",
                "description": "Whether or not to normalize the target values y by removing the mean\nand scaling to unit-variance. This is recommended for cases where\nzero-mean, unit-variance priors are used. Note that, in this\nimplementation, the normalisation is reversed before the GP predictions\nare reported.\n\nChanged in version 0.23.\n\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X_train",
                "description": "If True, a persistent copy of the training data is stored in the\nobject. Otherwise, just a reference to the training data is stored,\nwhich might cause predictions to change if the data is modified\nexternally.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "n_targets",
                "description": "The number of dimensions of the target values. Used to decide the number\nof outputs when sampling from the prior distributions (i.e. calling\nsample_y before fit). This parameter is ignored once\nfit has been called.\n\nNew in version 1.3.\n\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Determines random number generation used to initialize the centers.\nPass an int for reproducible results across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            }
        ],
        "references": [
            "[RW2006]\nCarl E. Rasmussen and Christopher K.I. Williams,\n\u201cGaussian Processes for Machine Learning\u201d,\nMIT Press 2006"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html"
    },
    "GradientBoostingRegressor": {
        "regressor_info": "Gradient Boosting for regression.\nThis estimator builds an additive model in a forward stage-wise fashion; it\nallows for the optimization of arbitrary differentiable loss functions. In\neach stage a regression tree is fit on the negative gradient of the given\nloss function.\nsklearn.ensemble.HistGradientBoostingRegressor is a much faster\nvariant of this algorithm for intermediate datasets (n_samples >= 10_000).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "loss",
                "description": "Loss function to be optimized. \u2018squared_error\u2019 refers to the squared\nerror for regression. \u2018absolute_error\u2019 refers to the absolute error of\nregression and is a robust loss function. \u2018huber\u2019 is a\ncombination of the two. \u2018quantile\u2019 allows quantile regression (use\nalpha to specify the quantile).\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, absolute_error, huber, quantile},             default=squared_error"
            },
            {
                "parameter": "learning_rate",
                "description": "Learning rate shrinks the contribution of each tree by learning_rate.\nThere is a trade-off between learning_rate and n_estimators.\nValues must be in the range [0.0, inf).\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "n_estimators",
                "description": "The number of boosting stages to perform. Gradient boosting\nis fairly robust to over-fitting so a large number usually\nresults in better performance.\nValues must be in the range [1, inf).\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "subsample",
                "description": "The fraction of samples to be used for fitting the individual base\nlearners. If smaller than 1.0 this results in Stochastic Gradient\nBoosting. subsample interacts with the parameter n_estimators.\nChoosing subsample < 1.0 leads to a reduction of variance\nand an increase in bias.\nValues must be in the range (0.0, 1.0].\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "criterion",
                "description": "The function to measure the quality of a split. Supported criteria are\n\u201cfriedman_mse\u201d for the mean squared error with improvement score by\nFriedman, \u201csquared_error\u201d for mean squared error. The default value of\n\u201cfriedman_mse\u201d is generally the best as it can provide a better\napproximation in some cases.\n\nNew in version 0.18.\n\n",
                "value_default": "friedman_mse",
                "data_type": "{friedman_mse, squared_error}"
            },
            {
                "parameter": "min_samples_split",
                "description": "The minimum number of samples required to split an internal node:\n\nIf int, values must be in the range [2, inf).\nIf float, values must be in the range (0.0, 1.0] and min_samples_split\nwill be ceil(min_samples_split * n_samples).\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "2",
                "data_type": "int or float"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast min_samples_leaf training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\nIf int, values must be in the range [1, inf).\nIf float, values must be in the range (0.0, 1.0) and min_samples_leaf\nwill be ceil(min_samples_leaf * n_samples).\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "1",
                "data_type": "int or float"
            },
            {
                "parameter": "min_weight_fraction_leaf",
                "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\nValues must be in the range [0.0, 0.5].\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_depth",
                "description": "Maximum depth of the individual regression estimators. The maximum\ndepth limits the number of nodes in the tree. Tune this parameter\nfor best performance; the best value depends on the interaction\nof the input variables. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\nIf int, values must be in the range [1, inf).\n",
                "value_default": "3",
                "data_type": "int or None"
            },
            {
                "parameter": "min_impurity_decrease",
                "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nValues must be in the range [0.0, inf).\nThe weighted impurity decrease equation is the following:\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n                    - N_t_L / N_t * left_impurity)\n\n\nwhere N is the total number of samples, N_t is the number of\nsamples at the current node, N_t_L is the number of samples in the\nleft child, and N_t_R is the number of samples in the right child.\nN, N_t, N_t_R and N_t_L all refer to the weighted sum,\nif sample_weight is passed.\n\nNew in version 0.19.\n\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "init",
                "description": "An estimator object that is used to compute the initial predictions.\ninit has to provide fit and predict. If \u2018zero\u2019, the\ninitial raw predictions are set to zero. By default a\nDummyEstimator is used, predicting either the average target value\n(for loss=\u2019squared_error\u2019), or a quantile for the other losses.\n",
                "value_default": "None",
                "data_type": "estimator or zero"
            },
            {
                "parameter": "random_state",
                "description": "Controls the random seed given to each Tree estimator at each\nboosting iteration.\nIn addition, it controls the random permutation of the features at\neach split (see Notes for more details).\nIt also controls the random splitting of the training data to obtain a\nvalidation set if n_iter_no_change is not None.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to consider when looking for the best split:\n\nIf int, values must be in the range [1, inf).\nIf float, values must be in the range (0.0, 1.0] and the features\nconsidered at each split will be max(1, int(max_features * n_features_in_)).\nIf \u201csqrt\u201d, then max_features=sqrt(n_features).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None, then max_features=n_features.\n\nChoosing max_features < n_features leads to a reduction of variance\nand an increase in bias.\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than max_features features.\n",
                "value_default": "None",
                "data_type": "{sqrt, log2}, int or float"
            },
            {
                "parameter": "alpha",
                "description": "The alpha-quantile of the huber loss function and the quantile\nloss function. Only if loss='huber' or loss='quantile'.\nValues must be in the range (0.0, 1.0).\n",
                "value_default": "0.9",
                "data_type": "float"
            },
            {
                "parameter": "verbose",
                "description": "Enable verbose output. If 1 then it prints progress and performance\nonce in a while (the more trees the lower the frequency). If greater\nthan 1 then it prints progress and performance for every tree.\nValues must be in the range [0, inf).\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "Grow trees with max_leaf_nodes in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nValues must be in the range [2, inf).\nIf None, then unlimited number of leaf nodes.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just erase the\nprevious solution. See the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "validation_fraction",
                "description": "The proportion of training data to set aside as validation set for\nearly stopping. Values must be in the range (0.0, 1.0).\nOnly used if n_iter_no_change is set to an integer.\n\nNew in version 0.20.\n\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "n_iter_no_change",
                "description": "n_iter_no_change is used to decide if early stopping will be used\nto terminate training when validation score is not improving. By\ndefault it is set to None to disable early stopping. If set to a\nnumber, it will set aside validation_fraction size of the training\ndata as validation and terminate training when validation score is not\nimproving in all of the previous n_iter_no_change numbers of\niterations.\nValues must be in the range [1, inf).\nSee\nEarly stopping in Gradient Boosting.\n\nNew in version 0.20.\n\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Tolerance for the early stopping. When the loss is not improving\nby at least tol for n_iter_no_change iterations (if set to a\nnumber), the training stops.\nValues must be in the range [0.0, inf).\n\nNew in version 0.20.\n\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "ccp_alpha",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\nccp_alpha will be chosen. By default, no pruning is performed.\nValues must be in the range [0.0, inf).\nSee Minimal Cost-Complexity Pruning for details.\n\nNew in version 0.22.\n\n",
                "value_default": "0.0",
                "data_type": "non-negative float"
            }
        ],
        "references": [
            "J. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.",
            "T. Hastie, R. Tibshirani and J. Friedman.\nElements of Statistical Learning Ed. 2, Springer, 2009."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
    },
    "HistGradientBoostingRegressor": {
        "regressor_info": "Histogram-based Gradient Boosting Regression Tree.\nThis estimator is much faster than\nGradientBoostingRegressor\nfor big datasets (n_samples >= 10 000).\nThis estimator has native support for missing values (NaNs). During\ntraining, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are\nassigned to the left or right child consequently. If no missing values\nwere encountered for a given feature during training, then samples with\nmissing values are mapped to whichever child has the most samples.\nThis implementation is inspired by\nLightGBM.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "loss",
                "description": "The loss function to use in the boosting process. Note that the\n\u201csquared error\u201d, \u201cgamma\u201d and \u201cpoisson\u201d losses actually implement\n\u201chalf least squares loss\u201d, \u201chalf gamma deviance\u201d and \u201chalf poisson\ndeviance\u201d to simplify the computation of the gradient. Furthermore,\n\u201cgamma\u201d and \u201cpoisson\u201d losses internally use a log-link, \u201cgamma\u201d\nrequires y > 0 and \u201cpoisson\u201d requires y >= 0.\n\u201cquantile\u201d uses the pinball loss.\n\nChanged in version 0.23: Added option \u2018poisson\u2019.\n\n\nChanged in version 1.1: Added option \u2018quantile\u2019.\n\n\nChanged in version 1.3: Added option \u2018gamma\u2019.\n\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, absolute_error, gamma, poisson, quantile},             default=squared_error"
            },
            {
                "parameter": "quantile",
                "description": "If loss is \u201cquantile\u201d, this parameter specifies which quantile to be estimated\nand must be between 0 and 1.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "learning_rate",
                "description": "The learning rate, also known as shrinkage. This is used as a\nmultiplicative factor for the leaves values. Use 1 for no\nshrinkage.\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations of the boosting process, i.e. the\nmaximum number of trees.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "The maximum number of leaves for each tree. Must be strictly greater\nthan 1. If None, there is no maximum limit.\n",
                "value_default": "31",
                "data_type": "int or None"
            },
            {
                "parameter": "max_depth",
                "description": "The maximum depth of each tree. The depth of a tree is the number of\nedges to go from the root to the deepest leaf.\nDepth isn\u2019t constrained by default.\n",
                "value_default": "None",
                "data_type": "int or None"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples per leaf. For small datasets with less\nthan a few hundred samples, it is recommended to lower this value\nsince only very shallow trees would be built.\n",
                "value_default": "20",
                "data_type": "int"
            },
            {
                "parameter": "l2_regularization",
                "description": "The L2 regularization parameter. Use 0 for no regularization (default).\n",
                "value_default": "0",
                "data_type": "float"
            },
            {
                "parameter": "max_bins",
                "description": "The maximum number of bins to use for non-missing values. Before\ntraining, each feature of the input array X is binned into\ninteger-valued bins, which allows for a much faster training stage.\nFeatures with a small number of unique values may use less than\nmax_bins bins. In addition to the max_bins bins, one more bin\nis always reserved for missing values. Must be no larger than 255.\n",
                "value_default": "255",
                "data_type": "int"
            },
            {
                "parameter": "categorical_features",
                "description": "Indicates the categorical features.\n\nNone : no feature will be considered categorical.\nboolean array-like : boolean mask indicating categorical features.\ninteger array-like : integer indices indicating categorical\nfeatures.\nstr array-like: names of categorical features (assuming the training\ndata has feature names).\n\"from_dtype\": dataframe columns with dtype \u201ccategory\u201d are\nconsidered to be categorical features. The input must be an object\nexposing a __dataframe__ method such as pandas or polars\nDataFrames to use this feature.\n\nFor each categorical feature, there must be at most max_bins unique\ncategories. Negative values for categorical features encoded as numeric\ndtypes are treated as missing values. All categorical values are\nconverted to floating point numbers. This means that categorical values\nof 1.0 and 1 are treated as the same category.\nRead more in the User Guide.\n\nNew in version 0.24.\n\n\nChanged in version 1.2: Added support for feature names.\n\n\nChanged in version 1.4: Added \"from_dtype\" option. The default will change to \"from_dtype\" in\nv1.6.\n\n",
                "value_default": "None",
                "data_type": "array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,)"
            },
            {
                "parameter": "monotonic_cst",
                "description": "Monotonic constraint to enforce on each feature are specified using the\nfollowing integer values:\n\n1: monotonic increase\n0: no constraint\n-1: monotonic decrease\n\nIf a dict with str keys, map feature to monotonic constraints by name.\nIf an array, the features are mapped to constraints by position. See\nUsing feature names to specify monotonic constraints for a usage example.\nRead more in the User Guide.\n\nNew in version 0.23.\n\n\nChanged in version 1.2: Accept dict of constraints with feature names as keys.\n\n",
                "value_default": "None",
                "data_type": "array-like of int of shape (n_features) or dict"
            },
            {
                "parameter": "interaction_cst",
                "description": "Specify interaction constraints, the sets of features which can\ninteract with each other in child node splits.\nEach item specifies the set of feature indices that are allowed\nto interact with each other. If there are more features than\nspecified in these constraints, they are treated as if they were\nspecified as an additional set.\nThe strings \u201cpairwise\u201d and \u201cno_interactions\u201d are shorthands for\nallowing only pairwise or no interactions, respectively.\nFor instance, with 5 features in total, interaction_cst=[{0, 1}]\nis equivalent to interaction_cst=[{0, 1}, {2, 3, 4}],\nand specifies that each branch of a tree will either only split\non features 0 and 1 or only split on features 2, 3 and 4.\n\nNew in version 1.2.\n\n",
                "value_default": "None",
                "data_type": "{pairwise, no_interactions} or sequence of lists/tuples/sets             of int"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble. For results to be valid, the\nestimator should be re-trained on the same data only.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "early_stopping",
                "description": "If \u2018auto\u2019, early stopping is enabled if the sample size is larger than\n10000. If True, early stopping is enabled, otherwise early stopping is\ndisabled.\n\nNew in version 0.23.\n\n",
                "value_default": "auto",
                "data_type": "auto or bool"
            },
            {
                "parameter": "scoring",
                "description": "Scoring parameter to use for early stopping. It can be a single\nstring (see The scoring parameter: defining model evaluation rules) or a callable (see\nDefining your scoring strategy from metric functions). If None, the estimator\u2019s default scorer is used. If\nscoring='loss', early stopping is checked w.r.t the loss value.\nOnly used if early stopping is performed.\n",
                "value_default": "loss",
                "data_type": "str or callable or None"
            },
            {
                "parameter": "validation_fraction",
                "description": "Proportion (or absolute size) of training data to set aside as\nvalidation data for early stopping. If None, early stopping is done on\nthe training data. Only used if early stopping is performed.\n",
                "value_default": "0.1",
                "data_type": "int or float or None"
            },
            {
                "parameter": "n_iter_no_change",
                "description": "Used to determine when to \u201cearly stop\u201d. The fitting process is\nstopped when none of the last n_iter_no_change scores are better\nthan the n_iter_no_change - 1 -th-to-last one, up to some\ntolerance. Only used if early stopping is performed.\n",
                "value_default": "10",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The absolute tolerance to use when comparing scores during early\nstopping. The higher the tolerance, the more likely we are to early\nstop: higher tolerance means that it will be harder for subsequent\niterations to be considered an improvement upon the reference score.\n",
                "value_default": "1e-7",
                "data_type": "float"
            },
            {
                "parameter": "verbose",
                "description": "The verbosity level. If not zero, print some information about the\nfitting process.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Pseudo-random number generator to control the subsampling in the\nbinning process, and the train/validation data split if early stopping\nis enabled.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html"
    },
    "HuberRegressor": {
        "regressor_info": "L2-regularized linear regression model that is robust to outliers.\nThe Huber Regressor optimizes the squared loss for the samples where\n|(y - Xw - c) / sigma| < epsilon and the absolute loss for the samples\nwhere |(y - Xw - c) / sigma| > epsilon, where the model coefficients\nw, the intercept c and the scale sigma are parameters\nto be optimized. The parameter sigma makes sure that if y is scaled up\nor down by a certain factor, one does not need to rescale epsilon to\nachieve the same robustness. Note that this does not take into account\nthe fact that the different features of X may be of different scales.\nThe Huber loss function has the advantage of not being heavily influenced\nby the outliers while not completely ignoring their effect.\nRead more in the User Guide",
        "parameters_info": [
            {
                "parameter": "epsilon",
                "description": "The parameter epsilon controls the number of samples that should be\nclassified as outliers. The smaller the epsilon, the more robust it is\nto outliers. Epsilon must be in the range [1, inf).\n",
                "value_default": "1.35",
                "data_type": "float"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations that\nscipy.optimize.minimize(method=\"L-BFGS-B\") should run for.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "alpha",
                "description": "Strength of the squared L2 regularization. Note that the penalty is\nequal to alpha * ||w||^2.\nMust be in the range [0, inf).\n",
                "value_default": "0.0001",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "This is useful if the stored attributes of a previously used model\nhas to be reused. If set to False, then the coefficients will\nbe rewritten for every call to fit.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether or not to fit the intercept. This can be set to False\nif the data is already centered around the origin.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "tol",
                "description": "The iteration will stop when\nmax{|proj g_i | i = 1, ..., n} <= tol\nwhere pg_i is the i-th component of the projected gradient.\n",
                "value_default": "1e-05",
                "data_type": "float"
            }
        ],
        "references": [
            "[1]\nPeter J. Huber, Elvezio M. Ronchetti, Robust Statistics\nConcomitant scale estimates, pg 172",
            "[2]\nArt B. Owen (2006), A robust hybrid of lasso and ridge regression.\nhttps://statweb.stanford.edu/~owen/reports/hhu.pdf"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html"
    },
    "IsotonicRegression": {
        "regressor_info": "Isotonic regression model.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "y_min",
                "description": "Lower bound on the lowest predicted value (the minimum value may\nstill be higher). If not set, defaults to -inf.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "y_max",
                "description": "Upper bound on the highest predicted value (the maximum may still be\nlower). If not set, defaults to +inf.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "increasing",
                "description": "Determines whether the predictions should be constrained to increase\nor decrease with X. \u2018auto\u2019 will decide based on the Spearman\ncorrelation estimate\u2019s sign.\n",
                "value_default": "True",
                "data_type": "bool or auto"
            },
            {
                "parameter": "out_of_bounds",
                "description": "Handles how X values outside of the training domain are handled\nduring prediction.\n\n\u2018nan\u2019, predictions will be NaN.\n\u2018clip\u2019, predictions will be set to the value corresponding to\nthe nearest train interval endpoint.\n\u2018raise\u2019, a ValueError is raised.\n\n",
                "value_default": "nan",
                "data_type": "{nan, clip, raise}"
            }
        ],
        "references": [
            "Isotonic Median Regression: A Linear Programming Approach\nNilotpal Chakravarti\nMathematics of Operations Research\nVol. 14, No. 2 (May, 1989), pp. 303-308",
            "Isotone Optimization in R : Pool-Adjacent-Violators\nAlgorithm (PAVA) and Active Set Methods\nde Leeuw, Hornik, Mair\nJournal of Statistical Software 2009",
            "Correctness of Kruskal\u2019s algorithms for monotone regression with ties\nde Leeuw, Psychometrica, 1977"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html"
    },
    "KNeighborsRegressor": {
        "regressor_info": "Regression based on k-nearest neighbors.\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_neighbors",
                "description": "Number of neighbors to use by default for kneighbors queries.\n",
                "value_default": "5",
                "data_type": "int"
            },
            {
                "parameter": "weights",
                "description": "Weight function used in prediction.  Possible values:\n\n\u2018uniform\u2019 : uniform weights.  All points in each neighborhood\nare weighted equally.\n\u2018distance\u2019 : weight points by the inverse of their distance.\nin this case, closer neighbors of a query point will have a\ngreater influence than neighbors which are further away.\n[callable] : a user-defined function which accepts an\narray of distances, and returns an array of the same shape\ncontaining the weights.\n\nUniform weights are used by default.\n",
                "value_default": "uniform",
                "data_type": "{uniform, distance}, callable or None"
            },
            {
                "parameter": "algorithm",
                "description": "Algorithm used to compute the nearest neighbors:\n\n\u2018ball_tree\u2019 will use BallTree\n\u2018kd_tree\u2019 will use KDTree\n\u2018brute\u2019 will use a brute-force search.\n\u2018auto\u2019 will attempt to decide the most appropriate algorithm\nbased on the values passed to fit method.\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force.\n",
                "value_default": "auto",
                "data_type": "{auto, ball_tree, kd_tree, brute}"
            },
            {
                "parameter": "leaf_size",
                "description": "Leaf size passed to BallTree or KDTree.  This can affect the\nspeed of the construction and query, as well as the memory\nrequired to store the tree.  The optimal value depends on the\nnature of the problem.\n",
                "value_default": "30",
                "data_type": "int"
            },
            {
                "parameter": "p",
                "description": "Power parameter for the Minkowski metric. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
                "value_default": "2",
                "data_type": "float"
            },
            {
                "parameter": "metric",
                "description": "Metric to use for distance computation. Default is \u201cminkowski\u201d, which\nresults in the standard Euclidean distance when p = 2. See the\ndocumentation of scipy.spatial.distance and\nthe metrics listed in\ndistance_metrics for valid metric\nvalues.\nIf metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and\nmust be square during fit. X may be a sparse graph, in which\ncase only \u201cnonzero\u201d elements may be considered neighbors.\nIf metric is a callable function, it takes two arrays representing 1D\nvectors as inputs and must return one value indicating the distance\nbetween those vectors. This works for Scipy\u2019s metrics, but is less\nefficient than passing the metric name as a string.\nIf metric is a DistanceMetric object, it will be passed directly to\nthe underlying computation routines.\n",
                "value_default": "minkowski",
                "data_type": "str, DistanceMetric object or callable"
            },
            {
                "parameter": "metric_params",
                "description": "Additional keyword arguments for the metric function.\n",
                "value_default": "None",
                "data_type": "dict"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of parallel jobs to run for neighbors search.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\nDoesn\u2019t affect fit method.\n",
                "value_default": "None",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"
    },
    "KernelRidge": {
        "regressor_info": "Kernel ridge regression.\nKernel ridge regression (KRR) combines ridge regression (linear least\nsquares with l2-norm regularization) with the kernel trick. It thus\nlearns a linear function in the space induced by the respective kernel and\nthe data. For non-linear kernels, this corresponds to a non-linear\nfunction in the original space.\nThe form of the model learned by KRR is identical to support vector\nregression (SVR). However, different loss functions are used: KRR uses\nsquared error loss while support vector regression uses epsilon-insensitive\nloss, both combined with l2 regularization. In contrast to SVR, fitting a\nKRR model can be done in closed-form and is typically faster for\nmedium-sized datasets. On the other hand, the learned model is non-sparse\nand thus slower than SVR, which learns a sparse model for epsilon > 0, at\nprediction-time.\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Regularization strength; must be a positive float. Regularization\nimproves the conditioning of the problem and reduces the variance of\nthe estimates. Larger values specify stronger regularization.\nAlpha corresponds to 1 / (2C) in other linear models such as\nLogisticRegression or\nLinearSVC. If an array is passed, penalties are\nassumed to be specific to the targets. Hence they must correspond in\nnumber. See Ridge regression and classification for formula.\n",
                "value_default": "1.0",
                "data_type": "float or array-like of shape (n_targets,)"
            },
            {
                "parameter": "kernel",
                "description": "Kernel mapping used internally. This parameter is directly passed to\npairwise_kernels.\nIf kernel is a string, it must be one of the metrics\nin pairwise.PAIRWISE_KERNEL_FUNCTIONS or \u201cprecomputed\u201d.\nIf kernel is \u201cprecomputed\u201d, X is assumed to be a kernel matrix.\nAlternatively, if kernel is a callable function, it is called on\neach pair of instances (rows) and the resulting value recorded. The\ncallable should take two rows from X as input and return the\ncorresponding kernel value as a single number. This means that\ncallables from sklearn.metrics.pairwise are not allowed, as\nthey operate on matrices, not single samples. Use the string\nidentifying the kernel instead.\n",
                "value_default": "linear",
                "data_type": "str or callable"
            },
            {
                "parameter": "gamma",
                "description": "Gamma parameter for the RBF, laplacian, polynomial, exponential chi2\nand sigmoid kernels. Interpretation of the default value is left to\nthe kernel; see the documentation for sklearn.metrics.pairwise.\nIgnored by other kernels.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "degree",
                "description": "Degree of the polynomial kernel. Ignored by other kernels.\n",
                "value_default": "3",
                "data_type": "float"
            },
            {
                "parameter": "coef0",
                "description": "Zero coefficient for polynomial and sigmoid kernels.\nIgnored by other kernels.\n",
                "value_default": "1",
                "data_type": "float"
            },
            {
                "parameter": "kernel_params",
                "description": "Additional parameters (keyword arguments) for kernel function passed\nas callable object.\n",
                "value_default": "None",
                "data_type": "dict"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html"
    },
    "Lars": {
        "regressor_info": "Least Angle Regression model a.k.a. LAR.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram\nmatrix can also be passed as argument.\n",
                "value_default": "auto",
                "data_type": "bool, auto or array-like "
            },
            {
                "parameter": "n_nonzero_coefs",
                "description": "Target number of non-zero coefficients. Use np.inf for no limit.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "eps",
                "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Unlike the tol parameter in some iterative\noptimization-based algorithms, this parameter does not control\nthe tolerance of the optimization.\n",
                "value_default": "np.finfo(float).eps",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "fit_path",
                "description": "If True the full path is stored in the coef_path_ attribute.\nIf you compute the solution for a large problem or many targets,\nsetting fit_path to False will lead to a speedup, especially\nwith a small alpha.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "jitter",
                "description": "Upper bound on a uniform noise parameter to be added to the\ny values, to satisfy the model\u2019s assumption of\none-at-a-time computations. Might help with stability.\n\nNew in version 0.23.\n\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "random_state",
                "description": "Determines random number generation for jittering. Pass an int\nfor reproducible output across multiple function calls.\nSee Glossary. Ignored if jitter is None.\n\nNew in version 0.23.\n\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html"
    },
    "LarsCV": {
        "regressor_info": "Cross-validated Least Angle Regression model.\nSee glossary entry for cross-validation estimator.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations to perform.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram matrix\ncannot be passed as argument since we will use only subsets of X.\n",
                "value_default": "auto",
                "data_type": "bool, auto or array-like "
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\ninteger, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or an iterable"
            },
            {
                "parameter": "max_n_alphas",
                "description": "The maximum number of points on the path used to compute the\nresiduals in the cross-validation.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int or None"
            },
            {
                "parameter": "eps",
                "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Unlike the tol parameter in some iterative\noptimization-based algorithms, this parameter does not control\nthe tolerance of the optimization.\n",
                "value_default": "np.finfo(float).eps",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LarsCV.html"
    },
    "Lasso": {
        "regressor_info": "Linear Model trained with L1 prior as regularizer (aka the Lasso).\nThe optimization objective for Lasso is:\nTechnically the Lasso model is optimizing the same objective function as\nthe Elastic Net with l1_ratio=1.0 (no L2 penalty).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L1 term, controlling regularization\nstrength. alpha must be a non-negative float i.e. in [0, inf).\nWhen alpha = 0, the objective is equivalent to ordinary least\nsquares, solved by the LinearRegression object. For numerical\nreasons, using alpha = 0 with the Lasso object is not advised.\nInstead, you should use the LinearRegression object.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto False, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. The Gram matrix can also be passed as argument.\nFor sparse input this option is always False to preserve sparsity.\n",
                "value_default": "False",
                "data_type": "bool or array-like of shape (n_features, n_features),                 default=False"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol, see Notes below.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "positive",
                "description": "When set to True, forces the coefficients to be positive.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
    },
    "LassoCV": {
        "regressor_info": "Lasso linear model with iterative fitting along a regularization path.\nSee glossary entry for cross-validation estimator.\nThe best model is selected by cross-validation.\nThe optimization objective for Lasso is:\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "eps",
                "description": "Length of the path. eps=1e-3 means that\nalpha_min / alpha_max = 1e-3.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "n_alphas",
                "description": "Number of alphas along the regularization path.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "alphas",
                "description": "List of alphas where to compute the models.\nIf None alphas are set automatically.\n",
                "value_default": "None",
                "data_type": "array-like"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram\nmatrix can also be passed as argument.\n",
                "value_default": "auto",
                "data_type": "auto, bool or array-like of shape             (n_features, n_features)"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\nint, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor int/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or iterable"
            },
            {
                "parameter": "verbose",
                "description": "Amount of verbosity.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "positive",
                "description": "If positive, restrict regression coefficients to be positive.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html"
    },
    "LassoLars": {
        "regressor_info": "Lasso model fit with Least Angle Regression a.k.a. Lars.\nIt is a Linear Model trained with an L1 prior as regularizer.\nThe optimization objective for Lasso is:\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the penalty term. Defaults to 1.0.\nalpha = 0 is equivalent to an ordinary least square, solved\nby LinearRegression. For numerical reasons, using\nalpha = 0 with the LassoLars object is not advised and you\nshould prefer the LinearRegression object.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram\nmatrix can also be passed as argument.\n",
                "value_default": "auto",
                "data_type": "bool, auto or array-like"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations to perform.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "eps",
                "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Unlike the tol parameter in some iterative\noptimization-based algorithms, this parameter does not control\nthe tolerance of the optimization.\n",
                "value_default": "np.finfo(float).eps",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "fit_path",
                "description": "If True the full path is stored in the coef_path_ attribute.\nIf you compute the solution for a large problem or many targets,\nsetting fit_path to False will lead to a speedup, especially\nwith a small alpha.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "positive",
                "description": "Restrict coefficients to be >= 0. Be aware that you might want to\nremove fit_intercept which is set True by default.\nUnder the positive restriction the model coefficients will not converge\nto the ordinary-least-squares solution for small values of alpha.\nOnly coefficients up to the smallest alpha value (alphas_[alphas_ >\n0.].min() when fit_path=True) reached by the stepwise Lars-Lasso\nalgorithm are typically in congruence with the solution of the\ncoordinate descent Lasso estimator.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "jitter",
                "description": "Upper bound on a uniform noise parameter to be added to the\ny values, to satisfy the model\u2019s assumption of\none-at-a-time computations. Might help with stability.\n\nNew in version 0.23.\n\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "random_state",
                "description": "Determines random number generation for jittering. Pass an int\nfor reproducible output across multiple function calls.\nSee Glossary. Ignored if jitter is None.\n\nNew in version 0.23.\n\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html"
    },
    "LassoLarsCV": {
        "regressor_info": "Cross-validated Lasso, using the LARS algorithm.\nSee glossary entry for cross-validation estimator.\nThe optimization objective for Lasso is:\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations to perform.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram matrix\ncannot be passed as argument since we will use only subsets of X.\n",
                "value_default": "auto",
                "data_type": "bool or auto "
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\ninteger, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or an iterable"
            },
            {
                "parameter": "max_n_alphas",
                "description": "The maximum number of points on the path used to compute the\nresiduals in the cross-validation.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int or None"
            },
            {
                "parameter": "eps",
                "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Unlike the tol parameter in some iterative\noptimization-based algorithms, this parameter does not control\nthe tolerance of the optimization.\n",
                "value_default": "np.finfo(float).eps",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "positive",
                "description": "Restrict coefficients to be >= 0. Be aware that you might want to\nremove fit_intercept which is set True by default.\nUnder the positive restriction the model coefficients do not converge\nto the ordinary-least-squares solution for small values of alpha.\nOnly coefficients up to the smallest alpha value (alphas_[alphas_ >\n0.].min() when fit_path=True) reached by the stepwise Lars-Lasso\nalgorithm are typically in congruence with the solution of the\ncoordinate descent Lasso estimator.\nAs a consequence using LassoLarsCV only makes sense for problems where\na sparse solution is expected and/or reached.\n",
                "value_default": "False",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html"
    },
    "LassoLarsIC": {
        "regressor_info": "Lasso model fit with Lars using BIC or AIC for model selection.\nThe optimization objective for Lasso is:\nAIC is the Akaike information criterion [2] and BIC is the Bayes\nInformation criterion [3]. Such criteria are useful to select the value\nof the regularization parameter by making a trade-off between the\ngoodness of fit and the complexity of the model. A good model should\nexplain well the data while being simple.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "criterion",
                "description": "The type of criterion to use.\n",
                "value_default": "aic",
                "data_type": "{aic, bic}"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram matrix to speed up\ncalculations. If set to 'auto' let us decide. The Gram\nmatrix can also be passed as argument.\n",
                "value_default": "auto",
                "data_type": "bool, auto or array-like"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations to perform. Can be used for\nearly stopping.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "eps",
                "description": "The machine-precision regularization in the computation of the\nCholesky diagonal factors. Increase this for very ill-conditioned\nsystems. Unlike the tol parameter in some iterative\noptimization-based algorithms, this parameter does not control\nthe tolerance of the optimization.\n",
                "value_default": "np.finfo(float).eps",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "positive",
                "description": "Restrict coefficients to be >= 0. Be aware that you might want to\nremove fit_intercept which is set True by default.\nUnder the positive restriction the model coefficients do not converge\nto the ordinary-least-squares solution for small values of alpha.\nOnly coefficients up to the smallest alpha value (alphas_[alphas_ >\n0.].min() when fit_path=True) reached by the stepwise Lars-Lasso\nalgorithm are typically in congruence with the solution of the\ncoordinate descent Lasso estimator.\nAs a consequence using LassoLarsIC only makes sense for problems where\na sparse solution is expected and/or reached.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "noise_variance",
                "description": "The estimated noise variance of the data. If None, an unbiased\nestimate is computed by an OLS model. However, it is only possible\nin the case where n_samples > n_features + fit_intercept.\n\nNew in version 1.1.\n\n",
                "value_default": "None",
                "data_type": "float"
            }
        ],
        "references": [
            "[1]\n(1,2)\nZou, Hui, Trevor Hastie, and Robert Tibshirani.\n\u201cOn the degrees of freedom of the lasso.\u201d\nThe Annals of Statistics 35.5 (2007): 2173-2192.",
            "[2]\nWikipedia entry on the Akaike information criterion",
            "[3]\nWikipedia entry on the Bayesian information criterion"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html"
    },
    "LinearRegression": {
        "regressor_info": "Ordinary least squares Linear Regression.\nLinearRegression fits a linear model with coefficients w = (w1, \u2026, wp)\nto minimize the residual sum of squares between the observed targets in\nthe dataset, and the targets predicted by the linear approximation.",
        "parameters_info": [
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto False, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of jobs to use for the computation. This will only provide\nspeedup in case of sufficiently large problems, that is if firstly\nn_targets > 1 and secondly X is sparse or if positive is set\nto True. None means 1 unless in a\njoblib.parallel_backend context. -1 means using all\nprocessors. See Glossary for more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "positive",
                "description": "When set to True, forces the coefficients to be positive. This\noption is only supported for dense arrays.\n\nNew in version 0.24.\n\n",
                "value_default": "False",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
    },
    "LinearSVR": {
        "regressor_info": "Linear Support Vector Regression.\nSimilar to SVR with parameter kernel=\u2019linear\u2019, but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\nThe main differences between LinearSVR and\nSVR lie in the loss function used by default, and in\nthe handling of intercept regularization between those two implementations.\nThis class supports both dense and sparse input.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "epsilon",
                "description": "Epsilon parameter in the epsilon-insensitive loss function. Note\nthat the value of this parameter depends on the scale of the target\nvariable y. If unsure, set epsilon=0.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "tol",
                "description": "Tolerance for stopping criteria.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "C",
                "description": "Regularization parameter. The strength of the regularization is\ninversely proportional to C. Must be strictly positive.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "loss",
                "description": "Specifies the loss function. The epsilon-insensitive loss\n(standard SVR) is the L1 loss, while the squared epsilon-insensitive\nloss (\u2018squared_epsilon_insensitive\u2019) is the L2 loss.\n",
                "value_default": "epsilon_insensitive",
                "data_type": "{epsilon_insensitive, squared_epsilon_insensitive},             default=epsilon_insensitive"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether or not to fit an intercept. If set to True, the feature vector\nis extended to include an intercept term: [x_1, ..., x_n, 1], where\n1 corresponds to the intercept. If set to False, no intercept will be\nused in calculations (i.e. data is expected to be already centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "intercept_scaling",
                "description": "When fit_intercept is True, the instance vector x becomes [x_1, ...,\nx_n, intercept_scaling], i.e. a \u201csynthetic\u201d feature with a constant\nvalue equal to intercept_scaling is appended to the instance vector.\nThe intercept becomes intercept_scaling * synthetic feature weight.\nNote that liblinear internally penalizes the intercept, treating it\nlike any other term in the feature vector. To reduce the impact of the\nregularization on the intercept, the intercept_scaling parameter can\nbe set to a value greater than 1; the higher the value of\nintercept_scaling, the lower the impact of regularization on it.\nThen, the weights become [w_x_1, ..., w_x_n,\nw_intercept*intercept_scaling], where w_x_1, ..., w_x_n represent\nthe feature weights and the intercept weight is scaled by\nintercept_scaling. This scaling allows the intercept term to have a\ndifferent regularization behavior compared to the other features.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "dual",
                "description": "Select the algorithm to either solve the dual or primal\noptimization problem. Prefer dual=False when n_samples > n_features.\ndual=\"auto\" will choose the value of the parameter automatically,\nbased on the values of n_samples, n_features and loss. If\nn_samples < n_features and optimizer supports chosen loss,\nthen dual will be set to True, otherwise it will be set to False.\n\nChanged in version 1.3: The \"auto\" option is added in version 1.3 and will be the default\nin version 1.5.\n\n",
                "value_default": "True",
                "data_type": "auto or bool"
            },
            {
                "parameter": "verbose",
                "description": "Enable verbose output. Note that this setting takes advantage of a\nper-process runtime setting in liblinear that, if enabled, may not work\nproperly in a multithreaded context.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Controls the pseudo random number generation for shuffling the data.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations to be run.\n",
                "value_default": "1000",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html"
    },
    "MLPRegressor": {
        "regressor_info": "Multi-layer Perceptron regressor.\nThis model optimizes the squared error using LBFGS or stochastic gradient\ndescent.",
        "parameters_info": [
            {
                "parameter": "hidden_layer_sizes",
                "description": "The ith element represents the number of neurons in the ith\nhidden layer.\n",
                "value_default": "(100,)",
                "data_type": "array-like of shape(n_layers - 2,)"
            },
            {
                "parameter": "activation",
                "description": "Activation function for the hidden layer.\n\n\u2018identity\u2019, no-op activation, useful to implement linear bottleneck,\nreturns f(x) = x\n\u2018logistic\u2019, the logistic sigmoid function,\nreturns f(x) = 1 / (1 + exp(-x)).\n\u2018tanh\u2019, the hyperbolic tan function,\nreturns f(x) = tanh(x).\n\u2018relu\u2019, the rectified linear unit function,\nreturns f(x) = max(0, x)\n\n",
                "value_default": "relu",
                "data_type": "{identity, logistic, tanh, relu}"
            },
            {
                "parameter": "solver",
                "description": "The solver for weight optimization.\n\n\u2018lbfgs\u2019 is an optimizer in the family of quasi-Newton methods.\n\u2018sgd\u2019 refers to stochastic gradient descent.\n\u2018adam\u2019 refers to a stochastic gradient-based optimizer proposed by\nKingma, Diederik, and Jimmy Ba\n\nNote: The default solver \u2018adam\u2019 works pretty well on relatively\nlarge datasets (with thousands of training samples or more) in terms of\nboth training time and validation score.\nFor small datasets, however, \u2018lbfgs\u2019 can converge faster and perform\nbetter.\n",
                "value_default": "adam",
                "data_type": "{lbfgs, sgd, adam}"
            },
            {
                "parameter": "alpha",
                "description": "Strength of the L2 regularization term. The L2 regularization term\nis divided by the sample size when added to the loss.\n",
                "value_default": "0.0001",
                "data_type": "float"
            },
            {
                "parameter": "batch_size",
                "description": "Size of minibatches for stochastic optimizers.\nIf the solver is \u2018lbfgs\u2019, the regressor will not use minibatch.\nWhen set to \u201cauto\u201d, batch_size=min(200, n_samples).\n",
                "value_default": "auto",
                "data_type": "int"
            },
            {
                "parameter": "learning_rate",
                "description": "Learning rate schedule for weight updates.\n\n\u2018constant\u2019 is a constant learning rate given by\n\u2018learning_rate_init\u2019.\n\u2018invscaling\u2019 gradually decreases the learning rate learning_rate_\nat each time step \u2018t\u2019 using an inverse scaling exponent of \u2018power_t\u2019.\neffective_learning_rate = learning_rate_init / pow(t, power_t)\n\u2018adaptive\u2019 keeps the learning rate constant to\n\u2018learning_rate_init\u2019 as long as training loss keeps decreasing.\nEach time two consecutive epochs fail to decrease training loss by at\nleast tol, or fail to increase validation score by at least tol if\n\u2018early_stopping\u2019 is on, the current learning rate is divided by 5.\n\nOnly used when solver=\u2019sgd\u2019.\n",
                "value_default": "constant",
                "data_type": "{constant, invscaling, adaptive}"
            },
            {
                "parameter": "learning_rate_init",
                "description": "The initial learning rate used. It controls the step-size\nin updating the weights. Only used when solver=\u2019sgd\u2019 or \u2018adam\u2019.\n",
                "value_default": "0.001",
                "data_type": "float"
            },
            {
                "parameter": "power_t",
                "description": "The exponent for inverse scaling learning rate.\nIt is used in updating effective learning rate when the learning_rate\nis set to \u2018invscaling\u2019. Only used when solver=\u2019sgd\u2019.\n",
                "value_default": "0.5",
                "data_type": "float"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations. The solver iterates until convergence\n(determined by \u2018tol\u2019) or this number of iterations. For stochastic\nsolvers (\u2018sgd\u2019, \u2018adam\u2019), note that this determines the number of epochs\n(how many times each data point will be used), not the number of\ngradient steps.\n",
                "value_default": "200",
                "data_type": "int"
            },
            {
                "parameter": "shuffle",
                "description": "Whether to shuffle samples in each iteration. Only used when\nsolver=\u2019sgd\u2019 or \u2018adam\u2019.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "Determines random number generation for weights and bias\ninitialization, train-test split if early stopping is used, and batch\nsampling when solver=\u2019sgd\u2019 or \u2018adam\u2019.\nPass an int for reproducible results across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "tol",
                "description": "Tolerance for the optimization. When the loss or score is not improving\nby at least tol for n_iter_no_change consecutive iterations,\nunless learning_rate is set to \u2018adaptive\u2019, convergence is\nconsidered to be reached and training stops.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "verbose",
                "description": "Whether to print progress messages to stdout.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous\ncall to fit as initialization, otherwise, just erase the\nprevious solution. See the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "momentum",
                "description": "Momentum for gradient descent update. Should be between 0 and 1. Only\nused when solver=\u2019sgd\u2019.\n",
                "value_default": "0.9",
                "data_type": "float"
            },
            {
                "parameter": "nesterovs_momentum",
                "description": "Whether to use Nesterov\u2019s momentum. Only used when solver=\u2019sgd\u2019 and\nmomentum > 0.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "early_stopping",
                "description": "Whether to use early stopping to terminate training when validation\nscore is not improving. If set to True, it will automatically set\naside validation_fraction of training data as validation and\nterminate training when validation score is not improving by at\nleast tol for n_iter_no_change consecutive epochs.\nOnly effective when solver=\u2019sgd\u2019 or \u2018adam\u2019.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "validation_fraction",
                "description": "The proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if early_stopping is True.\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "beta_1",
                "description": "Exponential decay rate for estimates of first moment vector in adam,\nshould be in [0, 1). Only used when solver=\u2019adam\u2019.\n",
                "value_default": "0.9",
                "data_type": "float"
            },
            {
                "parameter": "beta_2",
                "description": "Exponential decay rate for estimates of second moment vector in adam,\nshould be in [0, 1). Only used when solver=\u2019adam\u2019.\n",
                "value_default": "0.999",
                "data_type": "float"
            },
            {
                "parameter": "epsilon",
                "description": "Value for numerical stability in adam. Only used when solver=\u2019adam\u2019.\n",
                "value_default": "1e-8",
                "data_type": "float"
            },
            {
                "parameter": "n_iter_no_change",
                "description": "Maximum number of epochs to not meet tol improvement.\nOnly effective when solver=\u2019sgd\u2019 or \u2018adam\u2019.\n\nNew in version 0.20.\n\n",
                "value_default": "10",
                "data_type": "int"
            },
            {
                "parameter": "max_fun",
                "description": "Only used when solver=\u2019lbfgs\u2019. Maximum number of function calls.\nThe solver iterates until convergence (determined by tol), number\nof iterations reaches max_iter, or this number of function calls.\nNote that number of function calls will be greater than or equal to\nthe number of iterations for the MLPRegressor.\n\nNew in version 0.22.\n\n",
                "value_default": "15000",
                "data_type": "int"
            }
        ],
        "references": [
            "Hinton, Geoffrey E. \u201cConnectionist learning procedures.\u201d\nArtificial intelligence 40.1 (1989): 185-234.",
            "Glorot, Xavier, and Yoshua Bengio.\n\u201cUnderstanding the difficulty of training deep feedforward neural networks.\u201d\nInternational Conference on Artificial Intelligence and Statistics. 2010.",
            "He, Kaiming, et al (2015). \u201cDelving deep into rectifiers:\nSurpassing human-level performance on imagenet classification.\u201d",
            "Kingma, Diederik, and Jimmy Ba (2014)\n\u201cAdam: A method for stochastic optimization.\u201d"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"
    },
    "MultiTaskElasticNet": {
        "regressor_info": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\nThe optimization objective for MultiTaskElasticNet is:\nWhere:\ni.e. the sum of norms of each row.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "l1_ratio",
                "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\nis an L2 penalty.\nFor 0 < l1_ratio < 1, the penalty is a combination of L1/L2 and L2.\n",
                "value_default": "0.5",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html"
    },
    "MultiTaskElasticNetCV": {
        "regressor_info": "Multi-task L1/L2 ElasticNet with built-in cross-validation.\nSee glossary entry for cross-validation estimator.\nThe optimization objective for MultiTaskElasticNet is:\nWhere:\ni.e. the sum of norm of each row.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "l1_ratio",
                "description": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\nFor l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\nis an L2 penalty.\nFor 0 < l1_ratio < 1, the penalty is a combination of L1/L2 and L2.\nThis parameter can be a list, in which case the different\nvalues are tested by cross-validation and the one giving the best\nprediction score is used. Note that a good choice of list of\nvalues for l1_ratio is often to put more values close to 1\n(i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1, .5, .7,\n.9, .95, .99, 1].\n",
                "value_default": "0.5",
                "data_type": "float or list of float"
            },
            {
                "parameter": "eps",
                "description": "Length of the path. eps=1e-3 means that\nalpha_min / alpha_max = 1e-3.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "n_alphas",
                "description": "Number of alphas along the regularization path.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "alphas",
                "description": "List of alphas where to compute the models.\nIf not provided, set automatically.\n",
                "value_default": "None",
                "data_type": "array-like"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\nint, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor int/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or iterable"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "Amount of verbosity.\n",
                "value_default": "0",
                "data_type": "bool or int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation. Note that this is\nused only if multiple values for l1_ratio are given.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html"
    },
    "MultiTaskLasso": {
        "regressor_info": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\nThe optimization objective for Lasso is:\nWhere:\ni.e. the sum of norm of each row.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html"
    },
    "MultiTaskLassoCV": {
        "regressor_info": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\nSee glossary entry for cross-validation estimator.\nThe optimization objective for MultiTaskLasso is:\nWhere:\ni.e. the sum of norm of each row.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "eps",
                "description": "Length of the path. eps=1e-3 means that\nalpha_min / alpha_max = 1e-3.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "n_alphas",
                "description": "Number of alphas along the regularization path.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "alphas",
                "description": "List of alphas where to compute the models.\nIf not provided, set automatically.\n",
                "value_default": "None",
                "data_type": "array-like"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations.\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance for the optimization: if the updates are\nsmaller than tol, the optimization code checks the\ndual gap for optimality and continues until it is smaller\nthan tol.\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\nint, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor int/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or iterable"
            },
            {
                "parameter": "verbose",
                "description": "Amount of verbosity.\n",
                "value_default": "False",
                "data_type": "bool or int"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation. Note that this is\nused only if multiple values for l1_ratio are given.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "The seed of the pseudo random number generator that selects a random\nfeature to update. Used when selection == \u2018random\u2019.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "selection",
                "description": "If set to \u2018random\u2019, a random coefficient is updated every iteration\nrather than looping over features sequentially by default. This\n(setting to \u2018random\u2019) often leads to significantly faster convergence\nespecially when tol is higher than 1e-4.\n",
                "value_default": "cyclic",
                "data_type": "{cyclic, random}"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLassoCV.html"
    },
    "NuSVR": {
        "regressor_info": "Nu Support Vector Regression.\nSimilar to NuSVC, for regression, uses a parameter nu to control\nthe number of support vectors. However, unlike NuSVC, where nu\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR.\nThe implementation is based on libsvm.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "nu",
                "description": "An upper bound on the fraction of training errors and a lower bound of\nthe fraction of support vectors. Should be in the interval (0, 1].  By\ndefault 0.5 will be taken.\n",
                "value_default": "0.5",
                "data_type": "float"
            },
            {
                "parameter": "C",
                "description": "Penalty parameter C of the error term.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "kernel",
                "description": "Specifies the kernel type to be used in the algorithm.\nIf none is given, \u2018rbf\u2019 will be used. If a callable is given it is\nused to precompute the kernel matrix.\n",
                "value_default": "rbf",
                "data_type": "{linear, poly, rbf, sigmoid, precomputed} or callable,          default=rbf"
            },
            {
                "parameter": "degree",
                "description": "Degree of the polynomial kernel function (\u2018poly\u2019).\nMust be non-negative. Ignored by all other kernels.\n",
                "value_default": "3",
                "data_type": "int"
            },
            {
                "parameter": "gamma",
                "description": "Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n\nif gamma='scale' (default) is passed then it uses\n1 / (n_features * X.var()) as value of gamma,\nif \u2018auto\u2019, uses 1 / n_features\nif float, must be non-negative.\n\n\nChanged in version 0.22: The default value of gamma changed from \u2018auto\u2019 to \u2018scale\u2019.\n\n",
                "value_default": "scale",
                "data_type": "{scale, auto} or float"
            },
            {
                "parameter": "coef0",
                "description": "Independent term in kernel function.\nIt is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "shrinking",
                "description": "Whether to use the shrinking heuristic.\nSee the User Guide.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "tol",
                "description": "Tolerance for stopping criterion.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "cache_size",
                "description": "Specify the size of the kernel cache (in MB).\n",
                "value_default": "200",
                "data_type": "float"
            },
            {
                "parameter": "verbose",
                "description": "Enable verbose output. Note that this setting takes advantage of a\nper-process runtime setting in libsvm that, if enabled, may not work\nproperly in a multithreaded context.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "Hard limit on iterations within solver, or -1 for no limit.\n",
                "value_default": "-1",
                "data_type": "int"
            }
        ],
        "references": [
            "[1]\nLIBSVM: A Library for Support Vector Machines",
            "[2]\nPlatt, John (1999). \u201cProbabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods\u201d"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html"
    },
    "OrthogonalMatchingPursuit": {
        "regressor_info": "Orthogonal Matching Pursuit model (OMP).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_nonzero_coefs",
                "description": "Desired number of non-zero entries in the solution. If None (by\ndefault) this value is set to 10% of n_features.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "precompute",
                "description": "Whether to use a precomputed Gram and Xy matrix to speed up\ncalculations. Improves performance when n_targets or\nn_samples is very large. Note that if you already have such\nmatrices, you can pass them directly to the fit method.\n",
                "value_default": "auto",
                "data_type": "auto or bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html"
    },
    "OrthogonalMatchingPursuitCV": {
        "regressor_info": "Cross-validated Orthogonal Matching Pursuit model (OMP).\nSee glossary entry for cross-validation estimator.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "copy",
                "description": "Whether the design matrix X must be copied by the algorithm. A false\nvalue is only helpful if X is already Fortran-ordered, otherwise a\ncopy is made anyway.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum numbers of iterations to perform, therefore maximum features\nto include. 10% of n_features but at least 5 if available.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the default 5-fold cross-validation,\ninteger, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, KFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n\nChanged in version 0.22: cv default value if None changed from 3-fold to 5-fold.\n\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or iterable"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "verbose",
                "description": "Sets the verbosity amount.\n",
                "value_default": "False",
                "data_type": "bool or int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuitCV.html"
    },
    "PLSCanonical": {
        "regressor_info": "Partial Least Squares transformer and regressor.\nFor a comparison between other cross decomposition algorithms, see\nCompare cross decomposition methods.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_components",
                "description": "Number of components to keep. Should be in [1, min(n_samples,\nn_features, n_targets)].\n",
                "value_default": "2",
                "data_type": "int"
            },
            {
                "parameter": "scale",
                "description": "Whether to scale X and Y.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "algorithm",
                "description": "The algorithm used to estimate the first singular vectors of the\ncross-covariance matrix. \u2018nipals\u2019 uses the power method while \u2018svd\u2019\nwill compute the whole SVD.\n",
                "value_default": "nipals",
                "data_type": "{nipals, svd}"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations of the power method when\nalgorithm='nipals'. Ignored otherwise.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of u_i - u_{i-1} is less\nthan tol, where u corresponds to the left singular vector.\n",
                "value_default": "1e-06",
                "data_type": "float"
            },
            {
                "parameter": "copy",
                "description": "Whether to copy X and Y in fit before applying centering, and\npotentially scaling. If False, these operations will be done inplace,\nmodifying both arrays.\n",
                "value_default": "True",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSCanonical.html"
    },
    "PLSRegression": {
        "regressor_info": "PLS regression.\nPLSRegression is also known as PLS2 or PLS1, depending on the number of\ntargets.\nFor a comparison between other cross decomposition algorithms, see\nCompare cross decomposition methods.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_components",
                "description": "Number of components to keep. Should be in [1, min(n_samples,\nn_features, n_targets)].\n",
                "value_default": "2",
                "data_type": "int"
            },
            {
                "parameter": "scale",
                "description": "Whether to scale X and Y.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of iterations of the power method when\nalgorithm='nipals'. Ignored otherwise.\n",
                "value_default": "500",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The tolerance used as convergence criteria in the power method: the\nalgorithm stops whenever the squared norm of u_i - u_{i-1} is less\nthan tol, where u corresponds to the left singular vector.\n",
                "value_default": "1e-06",
                "data_type": "float"
            },
            {
                "parameter": "copy",
                "description": "Whether to copy X and Y in fit before applying centering,\nand potentially scaling. If False, these operations will be done\ninplace, modifying both arrays.\n",
                "value_default": "True",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html"
    },
    "PassiveAggressiveRegressor": {
        "regressor_info": "Passive Aggressive Regressor.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "C",
                "description": "Maximum step size (regularization). Defaults to 1.0.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether the intercept should be estimated or not. If False, the\ndata is assumed to be already centered. Defaults to True.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of passes over the training data (aka epochs).\nIt only impacts the behavior in the fit method, and not the\npartial_fit method.\n\nNew in version 0.19.\n\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The stopping criterion. If it is not None, the iterations will stop\nwhen (loss > previous_loss - tol).\n\nNew in version 0.19.\n\n",
                "value_default": "1e-3",
                "data_type": "float or None"
            },
            {
                "parameter": "early_stopping",
                "description": "Whether to use early stopping to terminate training when validation.\nscore is not improving. If set to True, it will automatically set aside\na fraction of training data as validation and terminate\ntraining when validation score is not improving by at least tol for\nn_iter_no_change consecutive epochs.\n\nNew in version 0.20.\n\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "validation_fraction",
                "description": "The proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if early_stopping is True.\n\nNew in version 0.20.\n\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "n_iter_no_change",
                "description": "Number of iterations with no improvement to wait before early stopping.\n\nNew in version 0.20.\n\n",
                "value_default": "5",
                "data_type": "int"
            },
            {
                "parameter": "shuffle",
                "description": "Whether or not the training data should be shuffled after each epoch.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "The verbosity level.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "loss",
                "description": "The loss function to be used:\nepsilon_insensitive: equivalent to PA-I in the reference paper.\nsquared_epsilon_insensitive: equivalent to PA-II in the reference\npaper.\n",
                "value_default": "epsilon_insensitive",
                "data_type": "str"
            },
            {
                "parameter": "epsilon",
                "description": "If the difference between the current prediction and the correct label\nis below this threshold, the model is not updated.\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "random_state",
                "description": "Used to shuffle the training data, when shuffle is set to\nTrue. Pass an int for reproducible output across multiple\nfunction calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\nRepeatedly calling fit or partial_fit when warm_start is True can\nresult in a different solution than when calling fit a single time\nbecause of the way the data is shuffled.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "average",
                "description": "When set to True, computes the averaged SGD weights and stores the\nresult in the coef_ attribute. If set to an int greater than 1,\naveraging will begin once the total number of samples seen reaches\naverage. So average=10 will begin averaging after seeing 10 samples.\n\nNew in version 0.19: parameter average to use weights averaging in SGD.\n\n",
                "value_default": "False",
                "data_type": "bool or int"
            }
        ],
        "references": [
            "Online Passive-Aggressive Algorithms\n<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\nK. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html"
    },
    "PoissonRegressor": {
        "regressor_info": "Generalized Linear Model with a Poisson distribution.\nThis regressor uses the \u2018log\u2019 link function.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L2 penalty term and determines the\nregularization strength. alpha = 0 is equivalent to unpenalized\nGLMs. In this case, the design matrix X must have full column rank\n(no collinearities).\nValues of alpha must be in the range [0.0, inf).\n",
                "value_default": "1",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Specifies if a constant (a.k.a. bias or intercept) should be\nadded to the linear predictor (X @ coef + intercept).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "solver",
                "description": "Algorithm to use in the optimization problem:\n\n\u2018lbfgs\u2019Calls scipy\u2019s L-BFGS-B optimizer.\n\n\u2018newton-cholesky\u2019Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\niterated reweighted least squares) with an inner Cholesky based solver.\nThis solver is a good choice for n_samples >> n_features, especially\nwith one-hot encoded categorical features with rare categories. Be aware\nthat the memory usage of this solver has a quadratic dependency on\nn_features because it explicitly computes the Hessian matrix.\n\nNew in version 1.2.\n\n\n\n",
                "value_default": "lbfgs",
                "data_type": "{lbfgs, newton-cholesky}"
            },
            {
                "parameter": "max_iter",
                "description": "The maximal number of iterations for the solver.\nValues must be in the range [1, inf).\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Stopping criterion. For the lbfgs solver,\nthe iteration will stop when max{|g_j|, j = 1, ..., d} <= tol\nwhere g_j is the j-th component of the gradient (derivative) of\nthe objective function.\nValues must be in the range (0.0, inf).\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "If set to True, reuse the solution of the previous call to fit\nas initialization for coef_ and intercept_ .\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "For the lbfgs solver set verbose to any positive number for verbosity.\nValues must be in the range [0, inf).\n",
                "value_default": "0",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html"
    },
    "QuantileRegressor": {
        "regressor_info": "Linear regression model that predicts conditional quantiles.\nThe linear QuantileRegressor optimizes the pinball loss for a\ndesired quantile and is robust to outliers.\nThis model uses an L1 regularization like\nLasso.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "quantile",
                "description": "The quantile that the model tries to predict. It must be strictly\nbetween 0 and 1. If 0.5 (default), the model predicts the 50%\nquantile, i.e. the median.\n",
                "value_default": "0.5",
                "data_type": "float"
            },
            {
                "parameter": "alpha",
                "description": "Regularization constant that multiplies the L1 penalty term.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether or not to fit the intercept.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "solver",
                "description": "Method used by scipy.optimize.linprog to solve the linear\nprogramming formulation.\nFrom scipy>=1.6.0, it is recommended to use the highs methods because\nthey are the fastest ones. Solvers \u201chighs-ds\u201d, \u201chighs-ipm\u201d and \u201chighs\u201d\nsupport sparse input data and, in fact, always convert to sparse csc.\nFrom scipy>=1.11.0, \u201cinterior-point\u201d is not available anymore.\n\nChanged in version 1.4: The default of solver changed to \"highs\" in version 1.4.\n\n",
                "value_default": "highs",
                "data_type": "{highs-ds, highs-ipm, highs, interior-point,             revised simplex}"
            },
            {
                "parameter": "solver_options",
                "description": "Additional parameters passed to scipy.optimize.linprog as\noptions. If None and if solver='interior-point', then\n{\"lstsq\": True} is passed to scipy.optimize.linprog for the\nsake of stability.\n",
                "value_default": "None",
                "data_type": "dict"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.QuantileRegressor.html"
    },
    "RANSACRegressor": {
        "regressor_info": "RANSAC (RANdom SAmple Consensus) algorithm.\nRANSAC is an iterative algorithm for the robust estimation of parameters\nfrom a subset of inliers from the complete data set.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "estimator",
                "description": "Base estimator object which implements the following methods:\n\n\nfit(X, y): Fit model to given training data and target values.\nscore(X, y): Returns the mean accuracy on the given test data,\nwhich is used for the stop criterion defined by stop_score.\nAdditionally, the score is used to decide which of two equally\nlarge consensus sets is chosen as the better one.\npredict(X): Returns predicted values using the linear model,\nwhich is used to compute residual error using loss function.\n\n\nIf estimator is None, then\nLinearRegression is used for\ntarget values of dtype float.\nNote that the current implementation only supports regression\nestimators.\n",
                "value_default": "None",
                "data_type": "object"
            },
            {
                "parameter": "min_samples",
                "description": "Minimum number of samples chosen randomly from original data. Treated\nas an absolute number of samples for min_samples >= 1, treated as a\nrelative number ceil(min_samples * X.shape[0]) for\nmin_samples < 1. This is typically chosen as the minimal number of\nsamples necessary to estimate the given estimator. By default a\nLinearRegression estimator is assumed and\nmin_samples is chosen as X.shape[1] + 1. This parameter is highly\ndependent upon the model, so if a estimator other than\nLinearRegression is used, the user must\nprovide a value.\n",
                "value_default": "None",
                "data_type": "int (>= 1) or float ([0, 1])"
            },
            {
                "parameter": "residual_threshold",
                "description": "Maximum residual for a data sample to be classified as an inlier.\nBy default the threshold is chosen as the MAD (median absolute\ndeviation) of the target values y. Points whose residuals are\nstrictly equal to the threshold are considered as inliers.\n",
                "value_default": "None",
                "data_type": "float"
            },
            {
                "parameter": "is_data_valid",
                "description": "This function is called with the randomly selected data before the\nmodel is fitted to it: is_data_valid(X, y). If its return value is\nFalse the current randomly chosen sub-sample is skipped.\n",
                "value_default": "None",
                "data_type": "callable"
            },
            {
                "parameter": "is_model_valid",
                "description": "This function is called with the estimated model and the randomly\nselected data: is_model_valid(model, X, y). If its return value is\nFalse the current randomly chosen sub-sample is skipped.\nRejecting samples with this function is computationally costlier than\nwith is_data_valid. is_model_valid should therefore only be used if\nthe estimated model is needed for making the rejection decision.\n",
                "value_default": "None",
                "data_type": "callable"
            },
            {
                "parameter": "max_trials",
                "description": "Maximum number of iterations for random sample selection.\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "max_skips",
                "description": "Maximum number of iterations that can be skipped due to finding zero\ninliers or invalid data defined by is_data_valid or invalid models\ndefined by is_model_valid.\n\nNew in version 0.19.\n\n",
                "value_default": "np.inf",
                "data_type": "int"
            },
            {
                "parameter": "stop_n_inliers",
                "description": "Stop iteration if at least this number of inliers are found.\n",
                "value_default": "np.inf",
                "data_type": "int"
            },
            {
                "parameter": "stop_score",
                "description": "Stop iteration if score is greater equal than this threshold.\n",
                "value_default": "np.inf",
                "data_type": "float"
            },
            {
                "parameter": "stop_probability",
                "description": "RANSAC iteration stops if at least one outlier-free set of the training\ndata is sampled in RANSAC. This requires to generate at least N\nsamples (iterations):\nN >= log(1 - probability) / log(1 - e**m)\n\n\nwhere the probability (confidence) is typically set to high value such\nas 0.99 (the default) and e is the current fraction of inliers w.r.t.\nthe total number of samples.\n",
                "value_default": "0.99",
                "data_type": "float in range [0, 1]"
            },
            {
                "parameter": "loss",
                "description": "String inputs, \u2018absolute_error\u2019 and \u2018squared_error\u2019 are supported which\nfind the absolute error and squared error per sample respectively.\nIf loss is a callable, then it should be a function that takes\ntwo arrays as inputs, the true and predicted value and returns a 1-D\narray with the i-th value of the array corresponding to the loss\non X[i].\nIf the loss on a sample is greater than the residual_threshold,\nthen this sample is classified as an outlier.\n\nNew in version 0.18.\n\n",
                "value_default": "absolute_error",
                "data_type": "str, callable"
            },
            {
                "parameter": "random_state",
                "description": "The generator used to initialize the centers.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            }
        ],
        "references": [
            "[1]\nhttps://en.wikipedia.org/wiki/RANSAC",
            "[2]\nhttps://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf",
            "[3]\nhttp://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html"
    },
    "RadiusNeighborsRegressor": {
        "regressor_info": "Regression based on neighbors within a fixed radius.\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "radius",
                "description": "Range of parameter space to use by default for radius_neighbors\nqueries.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "weights",
                "description": "Weight function used in prediction.  Possible values:\n\n\u2018uniform\u2019 : uniform weights.  All points in each neighborhood\nare weighted equally.\n\u2018distance\u2019 : weight points by the inverse of their distance.\nin this case, closer neighbors of a query point will have a\ngreater influence than neighbors which are further away.\n[callable] : a user-defined function which accepts an\narray of distances, and returns an array of the same shape\ncontaining the weights.\n\nUniform weights are used by default.\n",
                "value_default": "uniform",
                "data_type": "{uniform, distance}, callable or None"
            },
            {
                "parameter": "algorithm",
                "description": "Algorithm used to compute the nearest neighbors:\n\n\u2018ball_tree\u2019 will use BallTree\n\u2018kd_tree\u2019 will use KDTree\n\u2018brute\u2019 will use a brute-force search.\n\u2018auto\u2019 will attempt to decide the most appropriate algorithm\nbased on the values passed to fit method.\n\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force.\n",
                "value_default": "auto",
                "data_type": "{auto, ball_tree, kd_tree, brute}"
            },
            {
                "parameter": "leaf_size",
                "description": "Leaf size passed to BallTree or KDTree.  This can affect the\nspeed of the construction and query, as well as the memory\nrequired to store the tree.  The optimal value depends on the\nnature of the problem.\n",
                "value_default": "30",
                "data_type": "int"
            },
            {
                "parameter": "p",
                "description": "Power parameter for the Minkowski metric. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
                "value_default": "2",
                "data_type": "float"
            },
            {
                "parameter": "metric",
                "description": "Metric to use for distance computation. Default is \u201cminkowski\u201d, which\nresults in the standard Euclidean distance when p = 2. See the\ndocumentation of scipy.spatial.distance and\nthe metrics listed in\ndistance_metrics for valid metric\nvalues.\nIf metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and\nmust be square during fit. X may be a sparse graph, in which\ncase only \u201cnonzero\u201d elements may be considered neighbors.\nIf metric is a callable function, it takes two arrays representing 1D\nvectors as inputs and must return one value indicating the distance\nbetween those vectors. This works for Scipy\u2019s metrics, but is less\nefficient than passing the metric name as a string.\n",
                "value_default": "minkowski",
                "data_type": "str or callable"
            },
            {
                "parameter": "metric_params",
                "description": "Additional keyword arguments for the metric function.\n",
                "value_default": "None",
                "data_type": "dict"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of parallel jobs to run for neighbors search.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html"
    },
    "RandomForestRegressor": {
        "regressor_info": "A random forest regressor.\nA random forest is a meta estimator that fits a number of decision tree\nregressors on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nTrees in the forest use the best split strategy, i.e. equivalent to passing\nsplitter=\"best\" to the underlying DecisionTreeRegressor.\nThe sub-sample size is controlled with the max_samples parameter if\nbootstrap=True (default), otherwise the whole dataset is used to build\neach tree.\nFor a comparison between tree-based ensemble models see the example\nComparing Random Forests and Histogram Gradient Boosting models.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "n_estimators",
                "description": "The number of trees in the forest.\n\nChanged in version 0.22: The default value of n_estimators changed from 10 to 100\nin 0.22.\n\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "criterion",
                "description": "The function to measure the quality of a split. Supported criteria\nare \u201csquared_error\u201d for the mean squared error, which is equal to\nvariance reduction as feature selection criterion and minimizes the L2\nloss using the mean of each terminal node, \u201cfriedman_mse\u201d, which uses\nmean squared error with Friedman\u2019s improvement score for potential\nsplits, \u201cabsolute_error\u201d for the mean absolute error, which minimizes\nthe L1 loss using the median of each terminal node, and \u201cpoisson\u201d which\nuses reduction in Poisson deviance to find splits.\nTraining using \u201cabsolute_error\u201d is significantly slower\nthan when using \u201csquared_error\u201d.\n\nNew in version 0.18: Mean Absolute Error (MAE) criterion.\n\n\nNew in version 1.0: Poisson criterion.\n\n",
                "value_default": "squared_error",
                "data_type": "{squared_error, absolute_error, friedman_mse, poisson},             default=squared_error"
            },
            {
                "parameter": "max_depth",
                "description": "The maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_samples_split",
                "description": "The minimum number of samples required to split an internal node:\n\nIf int, then consider min_samples_split as the minimum number.\nIf float, then min_samples_split is a fraction and\nceil(min_samples_split * n_samples) are the minimum\nnumber of samples for each split.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "2",
                "data_type": "int or float"
            },
            {
                "parameter": "min_samples_leaf",
                "description": "The minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast min_samples_leaf training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\nIf float, then min_samples_leaf is a fraction and\nceil(min_samples_leaf * n_samples) are the minimum\nnumber of samples for each node.\n\n\nChanged in version 0.18: Added float values for fractions.\n\n",
                "value_default": "1",
                "data_type": "int or float"
            },
            {
                "parameter": "min_weight_fraction_leaf",
                "description": "The minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "max_features",
                "description": "The number of features to consider when looking for the best split:\n\nIf int, then consider max_features features at each split.\nIf float, then max_features is a fraction and\nmax(1, int(max_features * n_features_in_)) features are considered at each\nsplit.\nIf \u201csqrt\u201d, then max_features=sqrt(n_features).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None or 1.0, then max_features=n_features.\n\n\nNote\nThe default of 1.0 is equivalent to bagged trees and more\nrandomness can be achieved by setting smaller values, e.g. 0.3.\n\n\nChanged in version 1.1: The default of max_features changed from \"auto\" to 1.0.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than max_features features.\n",
                "value_default": "1.0",
                "data_type": "{sqrt, log2, None}, int or float"
            },
            {
                "parameter": "max_leaf_nodes",
                "description": "Grow trees with max_leaf_nodes in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "min_impurity_decrease",
                "description": "A node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\nThe weighted impurity decrease equation is the following:\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n                    - N_t_L / N_t * left_impurity)\n\n\nwhere N is the total number of samples, N_t is the number of\nsamples at the current node, N_t_L is the number of samples in the\nleft child, and N_t_R is the number of samples in the right child.\nN, N_t, N_t_R and N_t_L all refer to the weighted sum,\nif sample_weight is passed.\n\nNew in version 0.19.\n\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "bootstrap",
                "description": "Whether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "oob_score",
                "description": "Whether to use out-of-bag samples to estimate the generalization score.\nBy default, r2_score is used.\nProvide a callable with signature metric(y_true, y_pred) to use a\ncustom metric. Only available if bootstrap=True.\n",
                "value_default": "False",
                "data_type": "bool or callable"
            },
            {
                "parameter": "n_jobs",
                "description": "The number of jobs to run in parallel. fit, predict,\ndecision_path and apply are all parallelized over the\ntrees. None means 1 unless in a joblib.parallel_backend\ncontext. -1 means using all processors. See Glossary for more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "random_state",
                "description": "Controls both the randomness of the bootstrapping of the samples used\nwhen building trees (if bootstrap=True) and the sampling of the\nfeatures to consider when looking for the best split at each node\n(if max_features < n_features).\nSee Glossary for details.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "verbose",
                "description": "Controls the verbosity when fitting and predicting.\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See Glossary and\nFitting additional weak-learners for details.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "ccp_alpha",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\nccp_alpha will be chosen. By default, no pruning is performed. See\nMinimal Cost-Complexity Pruning for details.\n\nNew in version 0.22.\n\n",
                "value_default": "0.0",
                "data_type": "non-negative float"
            },
            {
                "parameter": "max_samples",
                "description": "If bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\nIf None (default), then draw X.shape[0] samples.\nIf int, then draw max_samples samples.\nIf float, then draw max(round(n_samples * max_samples), 1) samples. Thus,\nmax_samples should be in the interval (0.0, 1.0].\n\n\nNew in version 0.22.\n\n",
                "value_default": "None",
                "data_type": "int or float"
            }
        ],
        "references": [
            "[1]\n\nBreiman, \u201cRandom Forests\u201d, Machine Learning, 45(1), 5-32, 2001.",
            "[2]\nP. Geurts, D. Ernst., and L. Wehenkel, \u201cExtremely randomized\ntrees\u201d, Machine Learning, 63(1), 3-42, 2006."
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
    },
    "Ridge": {
        "regressor_info": "Linear least squares with l2 regularization.\nMinimizes the objective function:\nThis model solves a regression model where the loss function is\nthe linear least squares function and regularization is given by\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape (n_samples, n_targets)).\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L2 term, controlling regularization\nstrength. alpha must be a non-negative float i.e. in [0, inf).\nWhen alpha = 0, the objective is equivalent to ordinary least\nsquares, solved by the LinearRegression object. For numerical\nreasons, using alpha = 0 with the Ridge object is not advised.\nInstead, you should use the LinearRegression object.\nIf an array is passed, penalties are assumed to be specific to the\ntargets. Hence they must correspond in number.\n",
                "value_default": "1.0",
                "data_type": "{float, ndarray of shape (n_targets,)}"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to fit the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. X and y are expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations for conjugate gradient solver.\nFor \u2018sparse_cg\u2019 and \u2018lsqr\u2019 solvers, the default value is determined\nby scipy.sparse.linalg. For \u2018sag\u2019 solver, the default value is 1000.\nFor \u2018lbfgs\u2019 solver, the default value is 15000.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The precision of the solution (coef_) is determined by tol which\nspecifies a different convergence criterion for each solver:\n\n\u2018svd\u2019: tol has no impact.\n\u2018cholesky\u2019: tol has no impact.\n\u2018sparse_cg\u2019: norm of residuals smaller than tol.\n\u2018lsqr\u2019: tol is set as atol and btol of scipy.sparse.linalg.lsqr,\nwhich control the norm of the residual vector in terms of the norms of\nmatrix and coefficients.\n\u2018sag\u2019 and \u2018saga\u2019: relative change of coef smaller than tol.\n\u2018lbfgs\u2019: maximum of the absolute (projected) gradient=max|residuals|\nsmaller than tol.\n\n\nChanged in version 1.2: Default value changed from 1e-3 to 1e-4 for consistency with other linear\nmodels.\n\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "solver",
                "description": "Solver to use in the computational routines:\n\n\u2018auto\u2019 chooses the solver automatically based on the type of data.\n\u2018svd\u2019 uses a Singular Value Decomposition of X to compute the Ridge\ncoefficients. It is the most stable solver, in particular more stable\nfor singular matrices than \u2018cholesky\u2019 at the cost of being slower.\n\u2018cholesky\u2019 uses the standard scipy.linalg.solve function to\nobtain a closed-form solution.\n\u2018sparse_cg\u2019 uses the conjugate gradient solver as found in\nscipy.sparse.linalg.cg. As an iterative algorithm, this solver is\nmore appropriate than \u2018cholesky\u2019 for large-scale data\n(possibility to set tol and max_iter).\n\u2018lsqr\u2019 uses the dedicated regularized least-squares routine\nscipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\nprocedure.\n\u2018sag\u2019 uses a Stochastic Average Gradient descent, and \u2018saga\u2019 uses\nits improved, unbiased version named SAGA. Both methods also use an\niterative procedure, and are often faster than other solvers when\nboth n_samples and n_features are large. Note that \u2018sag\u2019 and\n\u2018saga\u2019 fast convergence is only guaranteed on features with\napproximately the same scale. You can preprocess the data with a\nscaler from sklearn.preprocessing.\n\u2018lbfgs\u2019 uses L-BFGS-B algorithm implemented in\nscipy.optimize.minimize. It can be used only when positive\nis True.\n\nAll solvers except \u2018svd\u2019 support both dense and sparse data. However, only\n\u2018lsqr\u2019, \u2018sag\u2019, \u2018sparse_cg\u2019, and \u2018lbfgs\u2019 support sparse input when\nfit_intercept is True.\n\nNew in version 0.17: Stochastic Average Gradient descent solver.\n\n\nNew in version 0.19: SAGA solver.\n\n",
                "value_default": "auto",
                "data_type": "{auto, svd, cholesky, lsqr, sparse_cg,             sag, saga, lbfgs}"
            },
            {
                "parameter": "positive",
                "description": "When set to True, forces the coefficients to be positive.\nOnly \u2018lbfgs\u2019 solver is supported in this case.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "random_state",
                "description": "Used when solver == \u2018sag\u2019 or \u2018saga\u2019 to shuffle the data.\nSee Glossary for details.\n\nNew in version 0.17: random_state to support Stochastic Average Gradient.\n\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
    },
    "RidgeCV": {
        "regressor_info": "Ridge regression with built-in cross-validation.\nSee glossary entry for cross-validation estimator.\nBy default, it performs efficient Leave-One-Out Cross-Validation.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "alphas",
                "description": "Array of alpha values to try.\nRegularization strength; must be a positive float. Regularization\nimproves the conditioning of the problem and reduces the variance of\nthe estimates. Larger values specify stronger regularization.\nAlpha corresponds to 1 / (2C) in other linear models such as\nLogisticRegression or\nLinearSVC.\nIf using Leave-One-Out cross-validation, alphas must be positive.\n",
                "value_default": "(0.1,1.0,10.0)",
                "data_type": "array-like of shape (n_alphas,)"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations\n(i.e. data is expected to be centered).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "scoring",
                "description": "A string (see model evaluation documentation) or\na scorer callable object / function with signature\nscorer(estimator, X, y).\nIf None, the negative mean squared error if cv is \u2018auto\u2019 or None\n(i.e. when using leave-one-out cross-validation), and r2 score\notherwise.\n",
                "value_default": "None",
                "data_type": "str, callable"
            },
            {
                "parameter": "cv",
                "description": "Determines the cross-validation splitting strategy.\nPossible inputs for cv are:\n\nNone, to use the efficient Leave-One-Out cross-validation\ninteger, to specify the number of folds.\nCV splitter,\nAn iterable yielding (train, test) splits as arrays of indices.\n\nFor integer/None inputs, if y is binary or multiclass,\nStratifiedKFold is used, else,\nKFold is used.\nRefer User Guide for the various\ncross-validation strategies that can be used here.\n",
                "value_default": "None",
                "data_type": "int, cross-validation generator or an iterable"
            },
            {
                "parameter": "gcv_mode",
                "description": "Flag indicating which strategy to use when performing\nLeave-One-Out Cross-Validation. Options are:\n'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n'svd' : force use of singular value decomposition of X when X is\n    dense, eigenvalue decomposition of X^T.X when X is sparse.\n'eigen' : force computation via eigendecomposition of X.X^T\n\n\nThe \u2018auto\u2019 mode is the default and is intended to pick the cheaper\noption of the two depending on the shape of the training data.\n",
                "value_default": "auto",
                "data_type": "{auto, svd, eigen}"
            },
            {
                "parameter": "store_cv_values",
                "description": "Flag indicating if the cross-validation values corresponding to\neach alpha should be stored in the cv_values_ attribute (see\nbelow). This flag is only compatible with cv=None (i.e. using\nLeave-One-Out Cross-Validation).\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "alpha_per_target",
                "description": "Flag indicating whether to optimize the alpha value (picked from the\nalphas parameter list) for each target separately (for multi-output\nsettings: multiple prediction targets). When set to True, after\nfitting, the alpha_ attribute will contain a value for each target.\nWhen set to False, a single alpha is used for all targets.\n\nNew in version 0.24.\n\n",
                "value_default": "False",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html"
    },
    "SGDRegressor": {
        "regressor_info": "Linear model fitted by minimizing a regularized empirical loss with SGD.\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na decreasing strength schedule (aka learning rate).\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\nThis implementation works with data represented as dense numpy arrays of\nfloating point values for the features.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "loss",
                "description": "The loss function to be used. The possible values are \u2018squared_error\u2019,\n\u2018huber\u2019, \u2018epsilon_insensitive\u2019, or \u2018squared_epsilon_insensitive\u2019\nThe \u2018squared_error\u2019 refers to the ordinary least squares fit.\n\u2018huber\u2019 modifies \u2018squared_error\u2019 to focus less on getting outliers\ncorrect by switching from squared to linear loss past a distance of\nepsilon. \u2018epsilon_insensitive\u2019 ignores errors less than epsilon and is\nlinear past that; this is the loss function used in SVR.\n\u2018squared_epsilon_insensitive\u2019 is the same but becomes squared loss past\na tolerance of epsilon.\nMore details about the losses formulas can be found in the\nUser Guide.\n",
                "value_default": "squared_error",
                "data_type": "str"
            },
            {
                "parameter": "penalty",
                "description": "The penalty (aka regularization term) to be used. Defaults to \u2018l2\u2019\nwhich is the standard regularizer for linear SVM models. \u2018l1\u2019 and\n\u2018elasticnet\u2019 might bring sparsity to the model (feature selection)\nnot achievable with \u2018l2\u2019. No penalty is added when set to None.\n",
                "value_default": "l2",
                "data_type": "{l2, l1, elasticnet, None}"
            },
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the regularization term. The higher the\nvalue, the stronger the regularization. Also used to compute the\nlearning rate when learning_rate is set to \u2018optimal\u2019.\nValues must be in the range [0.0, inf).\n",
                "value_default": "0.0001",
                "data_type": "float"
            },
            {
                "parameter": "l1_ratio",
                "description": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\nl1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\nOnly used if penalty is \u2018elasticnet\u2019.\nValues must be in the range [0.0, 1.0].\n",
                "value_default": "0.15",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Whether the intercept should be estimated or not. If False, the\ndata is assumed to be already centered.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "The maximum number of passes over the training data (aka epochs).\nIt only impacts the behavior in the fit method, and not the\npartial_fit method.\nValues must be in the range [1, inf).\n\nNew in version 0.19.\n\n",
                "value_default": "1000",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "The stopping criterion. If it is not None, training will stop\nwhen (loss > best_loss - tol) for n_iter_no_change consecutive\nepochs.\nConvergence is checked against the training loss or the\nvalidation loss depending on the early_stopping parameter.\nValues must be in the range [0.0, inf).\n\nNew in version 0.19.\n\n",
                "value_default": "1e-3",
                "data_type": "float or None"
            },
            {
                "parameter": "shuffle",
                "description": "Whether or not the training data should be shuffled after each epoch.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "The verbosity level.\nValues must be in the range [0, inf).\n",
                "value_default": "0",
                "data_type": "int"
            },
            {
                "parameter": "epsilon",
                "description": "Epsilon in the epsilon-insensitive loss functions; only if loss is\n\u2018huber\u2019, \u2018epsilon_insensitive\u2019, or \u2018squared_epsilon_insensitive\u2019.\nFor \u2018huber\u2019, determines the threshold at which it becomes less\nimportant to get the prediction exactly right.\nFor epsilon-insensitive, any differences between the current prediction\nand the correct label are ignored if they are less than this threshold.\nValues must be in the range [0.0, inf).\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "random_state",
                "description": "Used for shuffling the data, when shuffle is set to True.\nPass an int for reproducible output across multiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance"
            },
            {
                "parameter": "learning_rate",
                "description": "The learning rate schedule:\n\n\u2018constant\u2019: eta = eta0\n\u2018optimal\u2019: eta = 1.0 / (alpha * (t + t0))\nwhere t0 is chosen by a heuristic proposed by Leon Bottou.\n\u2018invscaling\u2019: eta = eta0 / pow(t, power_t)\n\u2018adaptive\u2019: eta = eta0, as long as the training keeps decreasing.\nEach time n_iter_no_change consecutive epochs fail to decrease the\ntraining loss by tol or fail to increase validation score by tol if\nearly_stopping is True, the current learning rate is divided by 5.\n\n\nNew in version 0.20: Added \u2018adaptive\u2019 option\n\n\n\n\n",
                "value_default": "invscaling",
                "data_type": "str"
            },
            {
                "parameter": "eta0",
                "description": "The initial learning rate for the \u2018constant\u2019, \u2018invscaling\u2019 or\n\u2018adaptive\u2019 schedules. The default value is 0.01.\nValues must be in the range [0.0, inf).\n",
                "value_default": "0.01",
                "data_type": "float"
            },
            {
                "parameter": "power_t",
                "description": "The exponent for inverse scaling learning rate.\nValues must be in the range (-inf, inf).\n",
                "value_default": "0.25",
                "data_type": "float"
            },
            {
                "parameter": "early_stopping",
                "description": "Whether to use early stopping to terminate training when validation\nscore is not improving. If set to True, it will automatically set aside\na fraction of training data as validation and terminate\ntraining when validation score returned by the score method is not\nimproving by at least tol for n_iter_no_change consecutive\nepochs.\n\nNew in version 0.20: Added \u2018early_stopping\u2019 option\n\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "validation_fraction",
                "description": "The proportion of training data to set aside as validation set for\nearly stopping. Must be between 0 and 1.\nOnly used if early_stopping is True.\nValues must be in the range (0.0, 1.0).\n\nNew in version 0.20: Added \u2018validation_fraction\u2019 option\n\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "n_iter_no_change",
                "description": "Number of iterations with no improvement to wait before stopping\nfitting.\nConvergence is checked against the training loss or the\nvalidation loss depending on the early_stopping parameter.\nInteger values must be in the range [1, max_iter).\n\nNew in version 0.20: Added \u2018n_iter_no_change\u2019 option\n\n",
                "value_default": "5",
                "data_type": "int"
            },
            {
                "parameter": "warm_start",
                "description": "When set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nSee the Glossary.\nRepeatedly calling fit or partial_fit when warm_start is True can\nresult in a different solution than when calling fit a single time\nbecause of the way the data is shuffled.\nIf a dynamic learning rate is used, the learning rate is adapted\ndepending on the number of samples already seen. Calling fit resets\nthis counter, while partial_fit  will result in increasing the\nexisting counter.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "average",
                "description": "When set to True, computes the averaged SGD weights across all\nupdates and stores the result in the coef_ attribute. If set to\nan int greater than 1, averaging will begin once the total number of\nsamples seen reaches average. So average=10 will begin\naveraging after seeing 10 samples.\n",
                "value_default": "False",
                "data_type": "bool or int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html"
    },
    "SVR": {
        "regressor_info": "Epsilon-Support Vector Regression.\nThe free parameters in the model are C and epsilon.\nThe implementation is based on libsvm. The fit time complexity\nis more than quadratic with the number of samples which makes it hard\nto scale to datasets with more than a couple of 10000 samples. For large\ndatasets consider using LinearSVR or\nSGDRegressor instead, possibly after a\nNystroem transformer or\nother Kernel Approximation.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "kernel",
                "description": "Specifies the kernel type to be used in the algorithm.\nIf none is given, \u2018rbf\u2019 will be used. If a callable is given it is\nused to precompute the kernel matrix.\n",
                "value_default": "rbf",
                "data_type": "{linear, poly, rbf, sigmoid, precomputed} or callable,          default=rbf"
            },
            {
                "parameter": "degree",
                "description": "Degree of the polynomial kernel function (\u2018poly\u2019).\nMust be non-negative. Ignored by all other kernels.\n",
                "value_default": "3",
                "data_type": "int"
            },
            {
                "parameter": "gamma",
                "description": "Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.\n\nif gamma='scale' (default) is passed then it uses\n1 / (n_features * X.var()) as value of gamma,\nif \u2018auto\u2019, uses 1 / n_features\nif float, must be non-negative.\n\n\nChanged in version 0.22: The default value of gamma changed from \u2018auto\u2019 to \u2018scale\u2019.\n\n",
                "value_default": "scale",
                "data_type": "{scale, auto} or float"
            },
            {
                "parameter": "coef0",
                "description": "Independent term in kernel function.\nIt is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n",
                "value_default": "0.0",
                "data_type": "float"
            },
            {
                "parameter": "tol",
                "description": "Tolerance for stopping criterion.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "C",
                "description": "Regularization parameter. The strength of the regularization is\ninversely proportional to C. Must be strictly positive.\nThe penalty is a squared l2 penalty.\n",
                "value_default": "1.0",
                "data_type": "float"
            },
            {
                "parameter": "epsilon",
                "description": "Epsilon in the epsilon-SVR model. It specifies the epsilon-tube\nwithin which no penalty is associated in the training loss function\nwith points predicted within a distance epsilon from the actual\nvalue. Must be non-negative.\n",
                "value_default": "0.1",
                "data_type": "float"
            },
            {
                "parameter": "shrinking",
                "description": "Whether to use the shrinking heuristic.\nSee the User Guide.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "cache_size",
                "description": "Specify the size of the kernel cache (in MB).\n",
                "value_default": "200",
                "data_type": "float"
            },
            {
                "parameter": "verbose",
                "description": "Enable verbose output. Note that this setting takes advantage of a\nper-process runtime setting in libsvm that, if enabled, may not work\nproperly in a multithreaded context.\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "max_iter",
                "description": "Hard limit on iterations within solver, or -1 for no limit.\n",
                "value_default": "-1",
                "data_type": "int"
            }
        ],
        "references": [
            "[1]\nLIBSVM: A Library for Support Vector Machines",
            "[2]\nPlatt, John (1999). \u201cProbabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods\u201d"
        ],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html"
    },
    "TheilSenRegressor": {
        "regressor_info": "Theil-Sen Estimator: robust multivariate regression model.\nThe algorithm calculates least square solutions on subsets with size\nn_subsamples of the samples in X. Any value of n_subsamples between the\nnumber of features and samples leads to an estimator with a compromise\nbetween robustness and efficiency. Since the number of least square\nsolutions is \u201cn_samples choose n_subsamples\u201d, it can be extremely large\nand can therefore be limited with max_subpopulation. If this limit is\nreached, the subsets are chosen randomly. In a final step, the spatial\nmedian (or L1 median) is calculated of all least square solutions.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "fit_intercept",
                "description": "Whether to calculate the intercept for this model. If set\nto false, no intercept will be used in calculations.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "copy_X",
                "description": "If True, X will be copied; else, it may be overwritten.\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "max_subpopulation",
                "description": "Instead of computing with a set of cardinality \u2018n choose k\u2019, where n is\nthe number of samples and k is the number of subsamples (at least\nnumber of features), consider only a stochastic subpopulation of a\ngiven maximal size if \u2018n choose k\u2019 is larger than max_subpopulation.\nFor other than small problem sizes this parameter will determine\nmemory usage and runtime if n_subsamples is not changed. Note that the\ndata type should be int but floats such as 1e4 can be accepted too.\n",
                "value_default": "1e4",
                "data_type": "int"
            },
            {
                "parameter": "n_subsamples",
                "description": "Number of samples to calculate the parameters. This is at least the\nnumber of features (plus 1 if fit_intercept=True) and the number of\nsamples as a maximum. A lower number leads to a higher breakdown\npoint and a low efficiency while a high number leads to a low\nbreakdown point and a high efficiency. If None, take the\nminimum number of subsamples leading to maximal robustness.\nIf n_subsamples is set to n_samples, Theil-Sen is identical to least\nsquares.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "max_iter",
                "description": "Maximum number of iterations for the calculation of spatial median.\n",
                "value_default": "300",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Tolerance when calculating spatial median.\n",
                "value_default": "1e-3",
                "data_type": "float"
            },
            {
                "parameter": "random_state",
                "description": "A random number generator instance to define the state of the random\npermutations generator. Pass an int for reproducible output across\nmultiple function calls.\nSee Glossary.\n",
                "value_default": "None",
                "data_type": "int, RandomState instance or None"
            },
            {
                "parameter": "n_jobs",
                "description": "Number of CPUs to use during the cross validation.\nNone means 1 unless in a joblib.parallel_backend context.\n-1 means using all processors. See Glossary\nfor more details.\n",
                "value_default": "None",
                "data_type": "int"
            },
            {
                "parameter": "verbose",
                "description": "Verbose mode when fitting the model.\n",
                "value_default": "False",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html"
    },
    "TransformedTargetRegressor": {
        "regressor_info": "Meta-estimator to regress on a transformed target.\nUseful for applying a non-linear transformation to the target y in\nregression problems. This transformation can be given as a Transformer\nsuch as the QuantileTransformer or as a\nfunction and its inverse such as np.log and np.exp.\nThe computation during fit is:\nor:\nThe computation during predict is:\nor:\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "regressor",
                "description": "Regressor object such as derived from\nRegressorMixin. This regressor will\nautomatically be cloned each time prior to fitting. If regressor is\nNone, LinearRegression is created and used.\n",
                "value_default": "None",
                "data_type": "object"
            },
            {
                "parameter": "transformer",
                "description": "Estimator object such as derived from\nTransformerMixin. Cannot be set at the same time\nas func and inverse_func. If transformer is None as well as\nfunc and inverse_func, the transformer will be an identity\ntransformer. Note that the transformer will be cloned during fitting.\nAlso, the transformer is restricting y to be a numpy array.\n",
                "value_default": "None",
                "data_type": "object"
            },
            {
                "parameter": "func",
                "description": "Function to apply to y before passing to fit. Cannot be set\nat the same time as transformer. The function needs to return a\n2-dimensional array. If func is None, the function used will be the\nidentity function.\n",
                "value_default": "None",
                "data_type": "function"
            },
            {
                "parameter": "inverse_func",
                "description": "Function to apply to the prediction of the regressor. Cannot be set at\nthe same time as transformer. The function needs to return a\n2-dimensional array. The inverse function is used to return\npredictions to the same space of the original training labels.\n",
                "value_default": "None",
                "data_type": "function"
            },
            {
                "parameter": "check_inverse",
                "description": "Whether to check that transform followed by inverse_transform\nor func followed by inverse_func leads to the original targets.\n",
                "value_default": "True",
                "data_type": "bool"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html"
    },
    "TweedieRegressor": {
        "regressor_info": "Generalized Linear Model with a Tweedie distribution.\nThis estimator can be used to model different GLMs depending on the\npower parameter, which determines the underlying distribution.\nRead more in the User Guide.",
        "parameters_info": [
            {
                "parameter": "power",
                "description": "The power determines the underlying target distribution according\nto the following table:\n\n\nPower\nDistribution\n\n\n\n0\nNormal\n\n1\nPoisson\n\n(1,2)\nCompound Poisson Gamma\n\n2\nGamma\n\n3\nInverse Gaussian\n\n\n\nFor 0 < power < 1, no distribution exists.\n",
                "value_default": "0",
                "data_type": "float"
            },
            {
                "parameter": "alpha",
                "description": "Constant that multiplies the L2 penalty term and determines the\nregularization strength. alpha = 0 is equivalent to unpenalized\nGLMs. In this case, the design matrix X must have full column rank\n(no collinearities).\nValues of alpha must be in the range [0.0, inf).\n",
                "value_default": "1",
                "data_type": "float"
            },
            {
                "parameter": "fit_intercept",
                "description": "Specifies if a constant (a.k.a. bias or intercept) should be\nadded to the linear predictor (X @ coef + intercept).\n",
                "value_default": "True",
                "data_type": "bool"
            },
            {
                "parameter": "link",
                "description": "The link function of the GLM, i.e. mapping from linear predictor\nX @ coeff + intercept to prediction y_pred. Option \u2018auto\u2019 sets\nthe link depending on the chosen power parameter as follows:\n\n\u2018identity\u2019 for power <= 0, e.g. for the Normal distribution\n\u2018log\u2019 for power > 0, e.g. for Poisson, Gamma and Inverse Gaussian\ndistributions\n\n",
                "value_default": "auto",
                "data_type": "{auto, identity, log}"
            },
            {
                "parameter": "solver",
                "description": "Algorithm to use in the optimization problem:\n\n\u2018lbfgs\u2019Calls scipy\u2019s L-BFGS-B optimizer.\n\n\u2018newton-cholesky\u2019Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\niterated reweighted least squares) with an inner Cholesky based solver.\nThis solver is a good choice for n_samples >> n_features, especially\nwith one-hot encoded categorical features with rare categories. Be aware\nthat the memory usage of this solver has a quadratic dependency on\nn_features because it explicitly computes the Hessian matrix.\n\nNew in version 1.2.\n\n\n\n",
                "value_default": "lbfgs",
                "data_type": "{lbfgs, newton-cholesky}"
            },
            {
                "parameter": "max_iter",
                "description": "The maximal number of iterations for the solver.\nValues must be in the range [1, inf).\n",
                "value_default": "100",
                "data_type": "int"
            },
            {
                "parameter": "tol",
                "description": "Stopping criterion. For the lbfgs solver,\nthe iteration will stop when max{|g_j|, j = 1, ..., d} <= tol\nwhere g_j is the j-th component of the gradient (derivative) of\nthe objective function.\nValues must be in the range (0.0, inf).\n",
                "value_default": "1e-4",
                "data_type": "float"
            },
            {
                "parameter": "warm_start",
                "description": "If set to True, reuse the solution of the previous call to fit\nas initialization for coef_ and intercept_ .\n",
                "value_default": "False",
                "data_type": "bool"
            },
            {
                "parameter": "verbose",
                "description": "For the lbfgs solver set verbose to any positive number for verbosity.\nValues must be in the range [0, inf).\n",
                "value_default": "0",
                "data_type": "int"
            }
        ],
        "references": [],
        "url_scrapped": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html"
    }
}